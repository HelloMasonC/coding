{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# 章节3-1 使用Urllib\n",
    "爬取CSDN（网址：https://edu.csdn.net/course/detail/29493 ）的一个课程页，并自动提取出QQ群。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "data = urllib.request.urlopen(\"https://edu.csdn.net/course/detail/29493\").read().decode('utf8')\n",
    "pat = '<span class=\"realname\" data-v-a39d224e>(.*?)</span>'\n",
    "result = re.compile(pat).findall(str(data))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节4-1 timeout设置\n",
    "循环爬取首页（网址:https://edu.hellobi.com/ ），0.5秒无响应超时异常。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# urllib.request.urlretrieve(\"https://edu.hellobi.com/\", filename=\"1.html\")\n",
    "# urllib.request.urlcleanup()\n",
    "for i in range(10):\n",
    "    try:\n",
    "        file = urllib.request.urlopen(\"https://edu.hellobi.com/\", timeout = 0.5)\n",
    "        print(file.getcode())\n",
    "    except Exception as e:\n",
    "        print(\"访问{0}出现异常：{1}\".format(file.geturl(), str(e)))\n",
    "# file.geturl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节4-2 网址有中文\n",
    "爬取百度（网址：http://www.baidu.com/ ）查找“Python”和“计算机”的查询结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "keywd1 = \"python\"\n",
    "keywd2 = urllib.request.quote(\"计算机\")\n",
    "url1 = \"http://www.baidu.com/s?wd=\" + keywd1\n",
    "url2 = \"http://www.baidu.com/s?wd=\" + keywd2\n",
    "req1 = urllib.request.Request(url1)\n",
    "data1 = urllib.request.urlopen(req1).read()\n",
    "req2= urllib.request.Request(url2)\n",
    "data2 = urllib.request.urlopen(req2).read()\n",
    "with open(\"1.html\", \"wb\") as f:\n",
    "    f.write(data1)\n",
    "with open(\"2.html\", \"wb\") as f:\n",
    "    f.write(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节4-3 使用post\n",
    "向网页（网址：http://www.iqianyue.com/mypost/ ）提交。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "url = \"http://www.iqianyue.com/mypost/\"\n",
    "mydata = urllib.parse.urlencode({\"name\":\"test@test.com\", \"pass\":\"123456jkl\"}).encode(\"utf-8\")\n",
    "req = urllib.request.Request(url, mydata)\n",
    "print(data, req)\n",
    "result = urllib.request.urlopen(req).read()\n",
    "print(result.decode(\"utf-8\"))\n",
    "# with open(\"1.html\", \"wb\") as fl:\n",
    "#     fl.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# 章节4-4 异常处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.error\n",
    "import urllib.request\n",
    "\n",
    "try:\n",
    "    result = urllib.request.urlopen(\"http://blog.csdn.net/ss12rwew\")\n",
    "except urllib.error.URLError as e:\n",
    "    if hasattr(e, \"code\"):\n",
    "        print(e.code)\n",
    "    if hasattr(e, \"reason\"):\n",
    "        print(e.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节4-5 使用urllib.request.urlretrieve\n",
    "爬取新浪新闻（网址：https://news.sina.com.cn/ ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "data = urllib.request.urlopen(\"https://news.sina.com.cn/\").read()\n",
    "data2 = data.decode(\"utf-8\", \"ignore\")\n",
    "pat = 'href=\"(https://news.sina.com.cn/.*?)\"'\n",
    "allurl = re.compile(pat).findall(data2)\n",
    "# print(allurl)\n",
    "for i in range(len(allurl)):\n",
    "    try:\n",
    "        print(\"第{}次爬取\".format(i+1))\n",
    "        file = \"sinanews/\" + str(i+1) + \".html\"\n",
    "        urllib.request.urlretrieve(allurl[i], file)\n",
    "        print(\"----成功----\")\n",
    "    except urllib.error.HTTPError as e:\n",
    "        print(str(e.code) + \": \" + e.reason)\n",
    "    except Exception as e:\n",
    "        print(\"失败：{}\".format(str(e)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节5-1 爬虫防屏蔽手段-代理服务器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "url = \"http://blog.csdn.net/\"\n",
    "headers = (\"user-agent\", \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36\")\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [headers]\n",
    "urllib.request.install_opener(opener)\n",
    "data = urllib.request.urlopen(url).read().decode(\"utf-8\", \"ignore\")\n",
    "print(\"output: \" + data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "def use_proxy(url, proxy_addr):\n",
    "    proxy = urllib.request.ProxyHandler({\"http\": proxy_addr})\n",
    "    opener = urllib.request.build_opener(proxy, urllib.request.HTTPHandler)\n",
    "    urllib.request.install_opener(opener)\n",
    "    data = urllib.request.urlopen(url).read().decode(\"utf-8\", \"ignore\")\n",
    "    return data\n",
    "\n",
    "proxy_addr = \"47.107.160.99:8118\"\n",
    "url = \"http://www.baidu.com\"\n",
    "data = use_proxy(url, proxy_addr)\n",
    "print(\"output: \" + data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "爬取快代理免费代理IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import random\n",
    "from lxml import etree\n",
    "from fake_useragent import UserAgent\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "def get_ip():\n",
    "    while True:\n",
    "        if not q.empty():\n",
    "        \t# 验证IP是否可用网址\n",
    "            url = 'http://httpbin.org/get'\n",
    "            proxies = q.get()\n",
    "            try:\n",
    "                html = requests.get(url, headers=headers, proxies=proxies, timeout=5).text\n",
    "                print('ip可以用')\n",
    "                with open('ip.txt','a')as f:\n",
    "                    f.write(str(proxies))\n",
    "                    f.write('\\n')\n",
    "            except:\n",
    "                print('ip不可用，下一个\\t')\n",
    "        else:\n",
    "            break\n",
    "\n",
    "def main():\n",
    "    t_list = []\n",
    "    for i in range(5):\n",
    "        t = Thread(target=get_ip)\n",
    "        t_list.append(t)\n",
    "        t.start()\n",
    "\n",
    "    for t in t_list:\n",
    "        t.join()\n",
    "\n",
    "\n",
    "ip_list = []\n",
    "q = Queue()\n",
    "# 爬取的是800到1000页   靠后的代理可能用的人少点，。自我安慰。。\n",
    "for i in range(1, 10):\n",
    "    url = 'https://www.kuaidaili.com/free/inha/{}'.format(i)\n",
    "    print(url)\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36',\n",
    "    }\n",
    "    # proxies = {'http': 'http://211.159.219.225:8118', 'https': 'https://211.159.219.225:8118'}\n",
    "    # html = requests.get(url, headers=headers,proxies=proxies).text\n",
    "    html = requests.get(url, headers=headers).text\n",
    "    # print(html)\n",
    "    parse_html = etree.HTML(html)\n",
    "    tr_list = parse_html.xpath('//*[@id=\"list\"]/table/tbody/tr')\n",
    "    # 延迟访问6到11秒。\n",
    "    sleep = random.randint(6, 11)\n",
    "    print(f'等待{sleep}秒')\n",
    "    time.sleep(sleep)\n",
    "    print('开始')\n",
    "    for tr in tr_list[1:]:\n",
    "        ip = tr.xpath('./td[1]/text()')[0]\n",
    "        port = tr.xpath('./td[2]/text()')[0]\n",
    "        proxies = {\n",
    "            'http': f'http://{ip}:{port}',\n",
    "            'https': f'https://{ip}:{port}',\n",
    "        }\n",
    "        print(proxies)\n",
    "        # 存入队列\n",
    "        q.put(proxies)\n",
    "    main()\n",
    "\n",
    "print(ip_list)\n",
    "print(\"----结束----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节5-2 图片爬虫\n",
    "把千图网（https://www.58pic.com/ ）某个频道的所有图片爬下来，高清原版的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "keyname=\"短裤\"\n",
    "key = urllib.request.quote(keyname)\n",
    "for i in range(0, 101):\n",
    "    url = \"https://movie.douban.com/top250?start={0}&filter=\".format(str(i*60))\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
