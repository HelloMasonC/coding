{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# 章节3-1 使用Urllib\n",
    "爬取CSDN（网址：https://edu.csdn.net/course/detail/29493 ）的一个课程页，并自动提取出QQ群。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "data = urllib.request.urlopen(\"https://edu.csdn.net/course/detail/29493\").read().decode('utf8')\n",
    "pat = '<span class=\"realname\" data-v-a39d224e>(.*?)</span>'\n",
    "result = re.compile(pat).findall(str(data))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节4-1 timeout设置\n",
    "循环爬取首页（网址:https://edu.hellobi.com/ ），0.5秒无响应超时异常。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# urllib.request.urlretrieve(\"https://edu.hellobi.com/\", filename=\"1.html\")\n",
    "# urllib.request.urlcleanup()\n",
    "for i in range(10):\n",
    "    try:\n",
    "        file = urllib.request.urlopen(\"https://edu.hellobi.com/\", timeout = 0.5)\n",
    "        print(file.getcode())\n",
    "    except Exception as e:\n",
    "        print(\"访问{0}出现异常：{1}\".format(file.geturl(), str(e)))\n",
    "# file.geturl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节4-2 网址有中文\n",
    "爬取百度（网址：http://www.baidu.com/ ）查找“Python”和“计算机”的查询结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "keywd1 = \"python\"\n",
    "keywd2 = urllib.request.quote(\"计算机\")\n",
    "url1 = \"http://www.baidu.com/s?wd=\" + keywd1\n",
    "url2 = \"http://www.baidu.com/s?wd=\" + keywd2\n",
    "req1 = urllib.request.Request(url1)\n",
    "data1 = urllib.request.urlopen(req1).read()\n",
    "req2= urllib.request.Request(url2)\n",
    "data2 = urllib.request.urlopen(req2).read()\n",
    "with open(\"1.html\", \"wb\") as f:\n",
    "    f.write(data1)\n",
    "with open(\"2.html\", \"wb\") as f:\n",
    "    f.write(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节4-3 使用post\n",
    "向网页（网址：http://www.iqianyue.com/mypost/ ）提交。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "url = \"http://www.iqianyue.com/mypost/\"\n",
    "mydata = urllib.parse.urlencode({\"name\":\"test@test.com\", \"pass\":\"123456jkl\"}).encode(\"utf-8\")\n",
    "req = urllib.request.Request(url, mydata)\n",
    "print(data, req)\n",
    "result = urllib.request.urlopen(req).read()\n",
    "print(result.decode(\"utf-8\"))\n",
    "# with open(\"1.html\", \"wb\") as fl:\n",
    "#     fl.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# 章节4-4 异常处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.error\n",
    "import urllib.request\n",
    "\n",
    "try:\n",
    "    result = urllib.request.urlopen(\"http://blog.csdn.net/ss12rwew\")\n",
    "except urllib.error.URLError as e:\n",
    "    if hasattr(e, \"code\"):\n",
    "        print(e.code)\n",
    "    if hasattr(e, \"reason\"):\n",
    "        print(e.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节4-5 使用urllib.request.urlretrieve\n",
    "爬取新浪新闻（网址：https://news.sina.com.cn/ ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "data = urllib.request.urlopen(\"https://news.sina.com.cn/\").read()\n",
    "data2 = data.decode(\"utf-8\", \"ignore\")\n",
    "pat = 'href=\"(https://news.sina.com.cn/.*?)\"'\n",
    "allurl = re.compile(pat).findall(data2)\n",
    "# print(allurl)\n",
    "for i in range(len(allurl)):\n",
    "    try:\n",
    "        print(\"第{}次爬取\".format(i+1))\n",
    "        file = \"sinanews/\" + str(i+1) + \".html\"\n",
    "        urllib.request.urlretrieve(allurl[i], file)\n",
    "        print(\"----成功----\")\n",
    "    except urllib.error.HTTPError as e:\n",
    "        print(str(e.code) + \": \" + e.reason)\n",
    "    except Exception as e:\n",
    "        print(\"失败：{}\".format(str(e)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# 章节5-1 爬虫防屏蔽手段-代理服务器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "url = \"http://blog.csdn.net/\"\n",
    "headers = (\"user-agent\", \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36\")\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [headers]\n",
    "urllib.request.install_opener(opener)\n",
    "data = urllib.request.urlopen(url).read().decode(\"utf-8\", \"ignore\")\n",
    "print(\"output: \" + data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "def use_proxy(url, proxy_addr):\n",
    "    proxy = urllib.request.ProxyHandler({\"http\": proxy_addr})\n",
    "    opener = urllib.request.build_opener(proxy, urllib.request.HTTPHandler)\n",
    "    urllib.request.install_opener(opener)\n",
    "    data = urllib.request.urlopen(url).read().decode(\"utf-8\", \"ignore\")\n",
    "    return data\n",
    "\n",
    "proxy_addr = \"47.107.160.99:8118\"\n",
    "url = \"http://www.baidu.com\"\n",
    "data = use_proxy(url, proxy_addr)\n",
    "print(\"output: \" + data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "爬取快代理免费代理IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import random\n",
    "from lxml import etree\n",
    "from fake_useragent import UserAgent\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "def get_ip():\n",
    "    while True:\n",
    "        if not q.empty():\n",
    "        \t# 验证IP是否可用网址\n",
    "            url = 'http://httpbin.org/get'\n",
    "            proxies = q.get()\n",
    "            try:\n",
    "                html = requests.get(url, headers=headers, proxies=proxies, timeout=5).text\n",
    "                print('ip可以用')\n",
    "                with open('ip.txt','a')as f:\n",
    "                    f.write(str(proxies))\n",
    "                    f.write('\\n')\n",
    "            except:\n",
    "                print('ip不可用，下一个\\t')\n",
    "        else:\n",
    "            break\n",
    "\n",
    "def main():\n",
    "    t_list = []\n",
    "    for i in range(5):\n",
    "        t = Thread(target=get_ip)\n",
    "        t_list.append(t)\n",
    "        t.start()\n",
    "\n",
    "    for t in t_list:\n",
    "        t.join()\n",
    "\n",
    "\n",
    "ip_list = []\n",
    "q = Queue()\n",
    "for i in range(1, 10):\n",
    "    url = 'https://www.kuaidaili.com/free/inha/{}'.format(i)\n",
    "    print(url)\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36',\n",
    "    }\n",
    "    # proxies = {'http': 'http://211.159.219.225:8118', 'https': 'https://211.159.219.225:8118'}\n",
    "    # html = requests.get(url, headers=headers,proxies=proxies).text\n",
    "    html = requests.get(url, headers=headers).text\n",
    "    # print(html)\n",
    "    parse_html = etree.HTML(html)\n",
    "    tr_list = parse_html.xpath('//*[@id=\"list\"]/table/tbody/tr')\n",
    "    # 延迟访问6到11秒。\n",
    "    sleep = random.randint(6, 11)\n",
    "    print(f'等待{sleep}秒')\n",
    "    time.sleep(sleep)\n",
    "    print('开始')\n",
    "    for tr in tr_list[1:]:\n",
    "        ip = tr.xpath('./td[1]/text()')[0]\n",
    "        port = tr.xpath('./td[2]/text()')[0]\n",
    "        proxies = {\n",
    "            'http': f'http://{ip}:{port}',\n",
    "            'https': f'https://{ip}:{port}',\n",
    "        }\n",
    "        print(proxies)\n",
    "        # 存入队列\n",
    "        q.put(proxies)\n",
    "    main()\n",
    "\n",
    "print(ip_list)\n",
    "print(\"----结束----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# 章节5-2 图片爬虫\n",
    "把千图网（https://www.58pic.com/ ）某个频道的所有图片爬下来，高清原版的。(网址变了)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "headers = (\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36\")\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders=[headers]\n",
    "urllib.request.install_opener(opener)\n",
    "\n",
    "for i in range(1,2):\n",
    "#     pageurl = \"https://www.58pic.com/haibaomoban/0/id-{}.html\".format(str(i*25))\n",
    "    pageurl = \"https://www.58pic.com/tupian/so-0-0-default-0-0-SO-0_10_0_0_0_0_0-0-{}.html\".format(str(i))\n",
    "    data = urllib.request.urlopen(pageurl).read().decode(\"utf-8\", \"ignore\")\n",
    "    pat = 'data-original=\"//(preview.qiantucdn.com/.*?new_nowater)\"'\n",
    "    imglist = re.compile(pat).findall(data)\n",
    "    for j in range(0, len(imglist)):\n",
    "        try:\n",
    "            thisimg = imglist[j]\n",
    "#             thisimgurl = thisimg + \"_1024.jpg\"\n",
    "            thisimgurl = \"http://\" + thisimg\n",
    "            file = \"img/\" + str(i) + str(j) + \".jpg\"\n",
    "            urllib.request.urlretrieve(thisimgurl, filename = file)\n",
    "            print(\"----爬取第{}页第{}个图片成功----\".format(str(i+1), str(j+1)))\n",
    "        except urllib.error.URLError as e:\n",
    "            if hasattr(e, \"code\"):\n",
    "                print(e.code)\n",
    "            if hasattr(e, \"reason\"):\n",
    "                print(e.reason)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "爬取豆瓣电影Top250（https://movie.douban.com/top250 ）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "# keyname=\"短裤\"\n",
    "# key = urllib.request.quote(keyname)\n",
    "headers = (\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36\")\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders=[headers]\n",
    "urllib.request.install_opener(opener)\n",
    "print(\"----开始爬取----\")\n",
    "for i in range(0, 10):\n",
    "    url = \"https://movie.douban.com/top250?start={0}&filter=\".format(str(i*25))\n",
    "    data = urllib.request.urlopen(url).read().decode(\"utf-8\", \"ignore\")\n",
    "    pat = 'src=\"(https://img.*?)\" class=\"\"'\n",
    "    imageurl_list = re.compile(pat).findall(data)\n",
    "    for j in range(len(imageurl_list)):\n",
    "        thisimg = imageurl_list[j]\n",
    "        file = \"img/\" + str(i) + str(j) + \".jpg\"\n",
    "        urllib.request.urlretrieve(thisimg, filename = file)\n",
    "    print(\"----爬取第{}页成功----\".format(str(i+1)))\n",
    "\n",
    "print(\"爬取网页列表为：\" + str(imageurl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# 章节6-1 抓包分析\n",
    "爬取腾讯视频（青云志）前20条评论（深度解读）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "headers = (\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36\")\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders=[headers]\n",
    "urllib.request.install_opener(opener)\n",
    "comid, total = \"\", 0\n",
    "url = 'https://video.coral.qq.com/filmreviewr/c/upcomment/g7jpq0qd0k8xdca?callback=_filmreviewrcupcommentg7jpq0qd0k8xdca&reqnum=3&source=132&commentid=' + comid + \"&_=1605598626157\"\n",
    "while total <= 20:\n",
    "    try:\n",
    "        data = urllib.request.urlopen(url, timeout = 10).read().decode()\n",
    "    except urllib.error.URLError as e:\n",
    "        if hasattr(e, \"code\"):\n",
    "            print(e.code)\n",
    "        if hasattr(e, \"reason\"):\n",
    "            print(e.reason)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    patnext = '\"last\":\"(.*?)\"'\n",
    "    nextid = re.compile(patnext).findall(data)[0]\n",
    "    pattitle = '\"title\":\"(.*?)\",'\n",
    "    comtitle = re.compile(pattitle).findall(data)\n",
    "    patcom = '\"content\":\"(.*?)\",'\n",
    "    comdata = re.compile(patcom).findall(data)\n",
    "    for j in range(len(comdata)):\n",
    "        total += 1\n",
    "        if total > 20:\n",
    "            break\n",
    "        print(\"----第{0}条评论----\".format(str(total)))\n",
    "        title = eval('u\"' + comtitle[j] + '\"') \n",
    "        print(\"标题：\" + title) \n",
    "        content = eval('u\"' + comdata[j] + '\"')\n",
    "        print(\"内容：\" + content)\n",
    "    url = 'https://video.coral.qq.com/filmreviewr/c/upcomment/g7jpq0qd0k8xdca?callback=_filmreviewrcupcommentg7jpq0qd0k8xdca&reqnum=3&source=132&commentid=' + nextid + \"&_=1605598626157\"\n",
    "print(\"----结束爬虫----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节6-3 微信爬虫实战\n",
    "使用fiddler代理服务器爬取微信网页（https://weixin.sogou.com/ ）信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "\n",
    "# 自定义函数，功能为使用代理服务器爬一个网址\n",
    "def use_proxy(proxy_addr, url):\n",
    "    try:\n",
    "        req = urllib.request.Request(url)\n",
    "        req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36')\n",
    "        proxy = urllib.request.ProxyHandler({'http': proxy_addr})\n",
    "        opener = urllib.request.build_opener(proxy, urllib.request.HTTPHandler)\n",
    "        urllib.request.install_opener(opener)\n",
    "        data = urllib.request.urlopen(req).read()\n",
    "        return data\n",
    "    except urllib.error.URLError as e:\n",
    "        if hasattr(e, 'code'):\n",
    "            print(e.code)\n",
    "        if hasattr(e, 'reason'):\n",
    "            print(e.reason)\n",
    "        time.sleep(10)\n",
    "    except Exception as e:\n",
    "        print(\"Exception:\" + str(e))\n",
    "        time.sleep(1)\n",
    "\n",
    "key = \"python\"\n",
    "proxy_addr = '127.0.0.1:8888'\n",
    "for i in range(0, 10):\n",
    "    key = urllib.request.quote(key)\n",
    "    thispageurl = \"https://weixin.sogou.com/weixin?type=2&query={}&page={}\".format(key, str(i))\n",
    "    thispagedata = use_proxy(proxy, thispageurl)\n",
    "    print(len(str(thispagedata)))\n",
    "    pat1 = '<a href=\"(.*?)\"'\n",
    "    rs1 = re.compile(pat1, re.S).findall(str(thispagedata))\n",
    "    if(len(rs1) == 0):\n",
    "        print(\"此次{}页没爬取成功。\".format(str(i)))\n",
    "        continue\n",
    "    for j in range(0, len(rs1)):\n",
    "        thisurl = rs1[j]\n",
    "        thisurl = thisurl.replace(\"amp;\", \"\")\n",
    "        file = \"/result/第{}页第{}篇文章.html\".format(str(i), str(j))\n",
    "        thisdata = use_proxy(proxy, thisurl)\n",
    "        try:\n",
    "            fh = open(file, \"wb\")\n",
    "            fh.write(thisdata)\n",
    "            fh.close()\n",
    "            print(\"第{}页第{}篇文章成功\".format(str(i), str(j)))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"第{}页第{}篇文章失败\".format(str(i), str(j)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS.微信改版，旧的代码已经不可用，新的请参考。https://www.cnblogs.com/hyonline/p/11812977.html\n",
    "涉及反爬技术：Cookie构造和js加密"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "from urllib import parse\n",
    "\n",
    "\n",
    "def get_cookie(response1, uigs_para, UserAgent):\n",
    "    SetCookie = response1.headers['Set-Cookie']\n",
    "    cookie_params = {\n",
    "        \"ABTEST\": re.findall('ABTEST=(.*?);', SetCookie, re.S)[0],\n",
    "        \"SNUID\": re.findall('SNUID=(.*?);', SetCookie, re.S)[0],\n",
    "        \"IPLOC\": re.findall('IPLOC=(.*?);', SetCookie, re.S)[0],\n",
    "        \"SUID\": re.findall('SUID=(.*?);', SetCookie, re.S)[0]\n",
    "    }\n",
    "    \n",
    "    url = \"https://www.sogou.com/sug/css/m3.min.v.7.css\"\n",
    "    headers = {\n",
    "        \"Accept\": \"text/css,*/*;q=0.1\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Cookie\": \"SNUID={}; IPLOC={}\".format(cookie_params['SNUID'], cookie_params['IPLOC']),\n",
    "        \"Host\": \"www.sogou.com\",\n",
    "        \"Referer\": \"https://weixin.sogou.com/\",\n",
    "        \"User-Agent\": UserAgent\n",
    "    }\n",
    "    response2 = requests.get(url, headers=headers)\n",
    "    SetCookie = response2.headers['Set-Cookie']\n",
    "    cookie_params['SUID'] = re.findall('SUID=(.*?);', SetCookie, re.S)[0]\n",
    "    \n",
    "    url = \"https://weixin.sogou.com/websearch/wexinurlenc_sogou_profile.jsp\"\n",
    "    headers = {\n",
    "        \"Accept\": \"*/*\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Cookie\": \"ABTEST={}; SNUID={}; IPLOC={}; SUID={}\".format(cookie_params['ABTEST'], cookie_params['SNUID'], cookie_params['IPLOC'],\n",
    "                                                                  cookie_params['SUID']),\n",
    "        \"Host\": \"weixin.sogou.com\",\n",
    "        \"Referer\": response1.url,\n",
    "        \"User-Agent\": UserAgent\n",
    "    }\n",
    "    response3 = requests.get(url, headers=headers)\n",
    "    SetCookie = response3.headers['Set-Cookie']\n",
    "    cookie_params['JSESSIONID'] = re.findall('JSESSIONID=(.*?);', SetCookie, re.S)[0]\n",
    "    \n",
    "    url = \"https://pb.sogou.com/pv.gif\"\n",
    "    headers = {\n",
    "        \"Accept\": \"image/webp,*/*\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Cookie\": \"SNUID={}; IPLOC={}; SUID={}\".format(cookie_params['SNUID'], cookie_params['IPLOC'], cookie_params['SUID']),\n",
    "        \"Host\": \"pb.sogou.com\",\n",
    "        \"Referer\": \"https://weixin.sogou.com/\",\n",
    "        \"User-Agent\": UserAgent\n",
    "    }\n",
    "    response4 = requests.get(url, headers=headers, params=uigs_para)\n",
    "    SetCookie = response4.headers['Set-Cookie']\n",
    "    cookie_params['SUV'] = re.findall('SUV=(.*?);', SetCookie, re.S)[0]\n",
    "    \n",
    "    return cookie_params\n",
    "\n",
    "\n",
    "def get_k_h(url):\n",
    "    b = int(random.random() * 100) + 1\n",
    "    a = url.find(\"url=\")\n",
    "    url = url + \"&k=\" + str(b) + \"&h=\" + url[a + 4 + 21 + b: a + 4 + 21 + b + 1]\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_uigs_para(response):\n",
    "    uigs_para = re.findall('var uigs_para = (.*?);', response.text, re.S)[0]\n",
    "    if 'passportUserId ? \"1\" : \"0\"' in uigs_para:\n",
    "        uigs_para = uigs_para.replace('passportUserId ? \"1\" : \"0\"', '0')\n",
    "    uigs_para = json.loads(uigs_para)\n",
    "    exp_id = re.findall('uigs_para.exp_id = \"(.*?)\";', response.text, re.S)[0]\n",
    "    uigs_para['right'] = 'right0_0'\n",
    "    uigs_para['exp_id'] = exp_id[:-1]\n",
    "    return uigs_para\n",
    "\n",
    "\n",
    "def main_v4(list_url, UserAgent):\n",
    "    headers1 = {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"zh-CN,zh;q=0.9,en;q=0.8\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Host\": \"weixin.sogou.com\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"User-Agent\": UserAgent,\n",
    "    }\n",
    "    response1 = requests.get(list_url, headers=headers1)\n",
    "    html = etree.HTML(response1.text)\n",
    "    urls = ['https://weixin.sogou.com' + i for i in html.xpath('//div[@class=\"img-box\"]/a/@href')]\n",
    "    \n",
    "    uigs_para = get_uigs_para(response1)\n",
    "    params = get_cookie(response1, uigs_para, UserAgent)\n",
    "    approve_url = 'https://weixin.sogou.com/approve?uuid={}'.format(uigs_para['uuid'])\n",
    "    headers2 = {\n",
    "        \"Accept\": \"*/*\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Cookie\": \"ABTEST={}; IPLOC={}; SUID={}; SUV={}; SNUID={}; JSESSIONID={};\".format(params['ABTEST'], params['IPLOC'],\n",
    "                                                                                          params['SUID'], params['SUV'], params['SNUID'],\n",
    "                                                                                          params['JSESSIONID']),\n",
    "        \"Host\": \"weixin.sogou.com\",\n",
    "        \"Referer\": response1.url,\n",
    "        \"User-Agent\": UserAgent,\n",
    "        \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "    }\n",
    "    for url in urls:\n",
    "        response2 = requests.get(approve_url, headers=headers2)\n",
    "        url = get_k_h(url)\n",
    "        headers3 = {\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "            \"Accept-Language\": \"zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Cookie\": \"ABTEST={}; SNUID={}; IPLOC={}; SUID={}; JSESSIONID={}; SUV={}\".format(params['ABTEST'], params['SNUID'],\n",
    "                                                                                             params['IPLOC'], params['SUID'],\n",
    "                                                                                             params['JSESSIONID'],\n",
    "                                                                                             params['SUV']),\n",
    "            \"Host\": \"weixin.sogou.com\",\n",
    "            \"Referer\": list_url,\n",
    "            \"Upgrade-Insecure-Requests\": \"1\",\n",
    "            \"User-Agent\": UserAgent\n",
    "        }\n",
    "        response3 = requests.get(url, headers=headers3)\n",
    "        \n",
    "        fragments = re.findall(\"url \\+= '(.*?)'\", response3.text, re.S)\n",
    "        itemurl = ''\n",
    "        for i in fragments:\n",
    "            itemurl += i\n",
    "        \n",
    "        # 文章url拿正文\n",
    "        headers4 = {\n",
    "            \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3\",\n",
    "            \"accept-encoding\": \"gzip, deflate, br\",\n",
    "            \"accept-language\": \"zh-CN,zh;q=0.9,en;q=0.8\",\n",
    "            \"cache-control\": \"max-age=0\",\n",
    "            \"user-agent\": UserAgent\n",
    "        }\n",
    "        response4 = requests.get(itemurl, headers=headers4)\n",
    "        html = etree.HTML(response4.text)\n",
    "        print(response4.status_code)\n",
    "        print(html.xpath('//meta[@property=\"og:title\"]/@content')[0])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    key = \"咸蛋超人\"\n",
    "    url = 'https://weixin.sogou.com/weixin?type=2&s_from=input&query={}&_sug_=n&_sug_type_=&page=1'.format(parse.quote(key))\n",
    "    UserAgent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:70.0) Gecko/20100101 Firefox/70.0\"\n",
    "    main_v4(url, UserAgent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节6-4 多线程爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "import urllib.error\n",
    "\n",
    "headers = (\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36\")\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders=[headers]\n",
    "urllib.request.install_opener(opener)\n",
    "for i in range(1,3):\n",
    "    url = \"https://www.qiushibaike.com/8hr/page/\" + str(i)\n",
    "    pagedata = urllib.request.urlopen(url).read().decode(\"utf-8\", \"ignore\")\n",
    "    pat = '<div class=\"content\">.*?<span>(.*?)</span>.*?</div>'\n",
    "    datalist = re.compile(pat, re.S).findall(pagedata)\n",
    "    for j in range(0, len(datalist)):\n",
    "        print(\"第\" + str(i) + \"页第\" + str(j) + \"个段子的内容是：\")\n",
    "        print(datalist[j])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是线程A\n",
      "我是线程A\n",
      "我是线程B\n",
      "我是线程B\n",
      "我是线程B\n",
      "我是线程B\n",
      "我是线程B\n",
      "我是线程B\n",
      "我是线程B\n",
      "我是线程B\n",
      "我是线程B\n",
      "我是线程B\n",
      "我是线程A\n",
      "我是线程A\n",
      "我是线程A\n",
      "我是线程A\n",
      "我是线程A\n",
      "我是线程A\n",
      "我是线程A\n",
      "我是线程A\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "class A(threading.Thread):\n",
    "    def __init__(self):\n",
    "        threading.Thread.__init__(self)\n",
    "    def run(self):\n",
    "        for i in range(10):\n",
    "            print(\"我是线程A\")\n",
    "class B(threading.Thread):\n",
    "    def __init__(self):\n",
    "        threading.Thread.__init__(self)\n",
    "    def run(self):\n",
    "        for i in range(10):\n",
    "            print(\"我是线程B\")\n",
    "t1 = A()\n",
    "t1.start()\n",
    "t2 = B()\n",
    "t2.start()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节6-5 Scrapy框架的安装\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
