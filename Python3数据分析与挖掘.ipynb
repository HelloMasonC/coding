{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# 章节3-1 使用Urllib\n",
    "爬取CSDN（网址：https://edu.csdn.net/course/detail/29493 ）的一个课程页，并自动提取出QQ群。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "data = urllib.request.urlopen(\"https://edu.csdn.net/course/detail/29493\").read().decode('utf8')\n",
    "pat = '<span class=\"realname\" data-v-a39d224e>(.*?)</span>'\n",
    "result = re.compile(pat).findall(str(data))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节4-1 timeout设置\n",
    "循环爬取首页（网址:https://edu.hellobi.com/ ），0.5秒无响应超时异常。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# urllib.request.urlretrieve(\"https://edu.hellobi.com/\", filename=\"1.html\")\n",
    "# urllib.request.urlcleanup()\n",
    "for i in range(10):\n",
    "    try:\n",
    "        file = urllib.request.urlopen(\"https://edu.hellobi.com/\", timeout = 0.5)\n",
    "        print(file.getcode())\n",
    "    except Exception as e:\n",
    "        print(\"访问{0}出现异常：{1}\".format(file.geturl(), str(e)))\n",
    "# file.geturl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节4-2 网址有中文\n",
    "爬取百度（网址：http://www.baidu.com/ ）查找“Python”和“计算机”的查询结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "keywd1 = \"python\"\n",
    "keywd2 = urllib.request.quote(\"计算机\")\n",
    "url1 = \"http://www.baidu.com/s?wd=\" + keywd1\n",
    "url2 = \"http://www.baidu.com/s?wd=\" + keywd2\n",
    "req1 = urllib.request.Request(url1)\n",
    "data1 = urllib.request.urlopen(req1).read()\n",
    "req2= urllib.request.Request(url2)\n",
    "data2 = urllib.request.urlopen(req2).read()\n",
    "with open(\"1.html\", \"wb\") as f:\n",
    "    f.write(data1)\n",
    "with open(\"2.html\", \"wb\") as f:\n",
    "    f.write(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节4-3 使用post\n",
    "向网页（网址：http://www.iqianyue.com/mypost/ ）提交。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "url = \"http://www.iqianyue.com/mypost/\"\n",
    "mydata = urllib.parse.urlencode({\"name\":\"test@test.com\", \"pass\":\"123456jkl\"}).encode(\"utf-8\")\n",
    "req = urllib.request.Request(url, mydata)\n",
    "print(data, req)\n",
    "result = urllib.request.urlopen(req).read()\n",
    "print(result.decode(\"utf-8\"))\n",
    "# with open(\"1.html\", \"wb\") as fl:\n",
    "#     fl.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# 章节4-4 异常处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.error\n",
    "import urllib.request\n",
    "\n",
    "try:\n",
    "    result = urllib.request.urlopen(\"http://blog.csdn.net/ss12rwew\")\n",
    "except urllib.error.URLError as e:\n",
    "    if hasattr(e, \"code\"):\n",
    "        print(e.code)\n",
    "    if hasattr(e, \"reason\"):\n",
    "        print(e.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节4-5 使用urllib.request.urlretrieve\n",
    "爬取新浪新闻（网址：https://news.sina.com.cn/ ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "data = urllib.request.urlopen(\"https://news.sina.com.cn/\").read()\n",
    "data2 = data.decode(\"utf-8\", \"ignore\")\n",
    "pat = 'href=\"(https://news.sina.com.cn/.*?)\"'\n",
    "allurl = re.compile(pat).findall(data2)\n",
    "# print(allurl)\n",
    "for i in range(len(allurl)):\n",
    "    try:\n",
    "        print(\"第{}次爬取\".format(i+1))\n",
    "        file = \"sinanews/\" + str(i+1) + \".html\"\n",
    "        urllib.request.urlretrieve(allurl[i], file)\n",
    "        print(\"----成功----\")\n",
    "    except urllib.error.HTTPError as e:\n",
    "        print(str(e.code) + \": \" + e.reason)\n",
    "    except Exception as e:\n",
    "        print(\"失败：{}\".format(str(e)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# 章节5-1 爬虫防屏蔽手段-代理服务器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "url = \"http://blog.csdn.net/\"\n",
    "headers = (\"user-agent\", \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36\")\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [headers]\n",
    "urllib.request.install_opener(opener)\n",
    "data = urllib.request.urlopen(url).read().decode(\"utf-8\", \"ignore\")\n",
    "print(\"output: \" + data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "def use_proxy(url, proxy_addr):\n",
    "    proxy = urllib.request.ProxyHandler({\"http\": proxy_addr})\n",
    "    opener = urllib.request.build_opener(proxy, urllib.request.HTTPHandler)\n",
    "    urllib.request.install_opener(opener)\n",
    "    data = urllib.request.urlopen(url).read().decode(\"utf-8\", \"ignore\")\n",
    "    return data\n",
    "\n",
    "proxy_addr = \"47.107.160.99:8118\"\n",
    "url = \"http://www.baidu.com\"\n",
    "data = use_proxy(url, proxy_addr)\n",
    "print(\"output: \" + data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "爬取快代理免费代理IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import random\n",
    "from lxml import etree\n",
    "from fake_useragent import UserAgent\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "def get_ip():\n",
    "    while True:\n",
    "        if not q.empty():\n",
    "        \t# 验证IP是否可用网址\n",
    "            url = 'http://httpbin.org/get'\n",
    "            proxies = q.get()\n",
    "            try:\n",
    "                html = requests.get(url, headers=headers, proxies=proxies, timeout=5).text\n",
    "                print('ip可以用')\n",
    "                with open('ip.txt','a')as f:\n",
    "                    f.write(str(proxies))\n",
    "                    f.write('\\n')\n",
    "            except:\n",
    "                print('ip不可用，下一个\\t')\n",
    "        else:\n",
    "            break\n",
    "\n",
    "def main():\n",
    "    t_list = []\n",
    "    for i in range(5):\n",
    "        t = Thread(target=get_ip)\n",
    "        t_list.append(t)\n",
    "        t.start()\n",
    "\n",
    "    for t in t_list:\n",
    "        t.join()\n",
    "\n",
    "\n",
    "ip_list = []\n",
    "q = Queue()\n",
    "for i in range(1, 10):\n",
    "    url = 'https://www.kuaidaili.com/free/inha/{}'.format(i)\n",
    "    print(url)\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36',\n",
    "    }\n",
    "    # proxies = {'http': 'http://211.159.219.225:8118', 'https': 'https://211.159.219.225:8118'}\n",
    "    # html = requests.get(url, headers=headers,proxies=proxies).text\n",
    "    html = requests.get(url, headers=headers).text\n",
    "    # print(html)\n",
    "    parse_html = etree.HTML(html)\n",
    "    tr_list = parse_html.xpath('//*[@id=\"list\"]/table/tbody/tr')\n",
    "    # 延迟访问6到11秒。\n",
    "    sleep = random.randint(6, 11)\n",
    "    print(f'等待{sleep}秒')\n",
    "    time.sleep(sleep)\n",
    "    print('开始')\n",
    "    for tr in tr_list[1:]:\n",
    "        ip = tr.xpath('./td[1]/text()')[0]\n",
    "        port = tr.xpath('./td[2]/text()')[0]\n",
    "        proxies = {\n",
    "            'http': f'http://{ip}:{port}',\n",
    "            'https': f'https://{ip}:{port}',\n",
    "        }\n",
    "        print(proxies)\n",
    "        # 存入队列\n",
    "        q.put(proxies)\n",
    "    main()\n",
    "\n",
    "print(ip_list)\n",
    "print(\"----结束----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# 章节5-2 图片爬虫\n",
    "把千图网（https://www.58pic.com/ ）某个频道的所有图片爬下来，高清原版的。(网址变了)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "headers = (\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36\")\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders=[headers]\n",
    "urllib.request.install_opener(opener)\n",
    "\n",
    "for i in range(1,2):\n",
    "#     pageurl = \"https://www.58pic.com/haibaomoban/0/id-{}.html\".format(str(i*25))\n",
    "    pageurl = \"https://www.58pic.com/tupian/so-0-0-default-0-0-SO-0_10_0_0_0_0_0-0-{}.html\".format(str(i))\n",
    "    data = urllib.request.urlopen(pageurl).read().decode(\"utf-8\", \"ignore\")\n",
    "    pat = 'data-original=\"//(preview.qiantucdn.com/.*?new_nowater)\"'\n",
    "    imglist = re.compile(pat).findall(data)\n",
    "    for j in range(0, len(imglist)):\n",
    "        try:\n",
    "            thisimg = imglist[j]\n",
    "#             thisimgurl = thisimg + \"_1024.jpg\"\n",
    "            thisimgurl = \"http://\" + thisimg\n",
    "            file = \"img/\" + str(i) + str(j) + \".jpg\"\n",
    "            urllib.request.urlretrieve(thisimgurl, filename = file)\n",
    "            print(\"----爬取第{}页第{}个图片成功----\".format(str(i+1), str(j+1)))\n",
    "        except urllib.error.URLError as e:\n",
    "            if hasattr(e, \"code\"):\n",
    "                print(e.code)\n",
    "            if hasattr(e, \"reason\"):\n",
    "                print(e.reason)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "爬取豆瓣电影Top250（https://movie.douban.com/top250 ）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "# keyname=\"短裤\"\n",
    "# key = urllib.request.quote(keyname)\n",
    "headers = (\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36\")\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders=[headers]\n",
    "urllib.request.install_opener(opener)\n",
    "print(\"----开始爬取----\")\n",
    "for i in range(0, 10):\n",
    "    url = \"https://movie.douban.com/top250?start={0}&filter=\".format(str(i*25))\n",
    "    data = urllib.request.urlopen(url).read().decode(\"utf-8\", \"ignore\")\n",
    "    pat = 'src=\"(https://img.*?)\" class=\"\"'\n",
    "    imageurl_list = re.compile(pat).findall(data)\n",
    "    for j in range(len(imageurl_list)):\n",
    "        thisimg = imageurl_list[j]\n",
    "        file = \"img/\" + str(i) + str(j) + \".jpg\"\n",
    "        urllib.request.urlretrieve(thisimg, filename = file)\n",
    "    print(\"----爬取第{}页成功----\".format(str(i+1)))\n",
    "\n",
    "print(\"爬取网页列表为：\" + str(imageurl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# 章节6-1 抓包分析\n",
    "爬取腾讯视频（青云志）前20条评论（深度解读）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "headers = (\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36\")\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders=[headers]\n",
    "urllib.request.install_opener(opener)\n",
    "comid, total = \"\", 0\n",
    "url = 'https://video.coral.qq.com/filmreviewr/c/upcomment/g7jpq0qd0k8xdca?callback=_filmreviewrcupcommentg7jpq0qd0k8xdca&reqnum=3&source=132&commentid=' + comid + \"&_=1605598626157\"\n",
    "while total <= 20:\n",
    "    try:\n",
    "        data = urllib.request.urlopen(url, timeout = 10).read().decode()\n",
    "    except urllib.error.URLError as e:\n",
    "        if hasattr(e, \"code\"):\n",
    "            print(e.code)\n",
    "        if hasattr(e, \"reason\"):\n",
    "            print(e.reason)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    patnext = '\"last\":\"(.*?)\"'\n",
    "    nextid = re.compile(patnext).findall(data)[0]\n",
    "    pattitle = '\"title\":\"(.*?)\",'\n",
    "    comtitle = re.compile(pattitle).findall(data)\n",
    "    patcom = '\"content\":\"(.*?)\",'\n",
    "    comdata = re.compile(patcom).findall(data)\n",
    "    for j in range(len(comdata)):\n",
    "        total += 1\n",
    "        if total > 20:\n",
    "            break\n",
    "        print(\"----第{0}条评论----\".format(str(total)))\n",
    "        title = eval('u\"' + comtitle[j] + '\"') \n",
    "        print(\"标题：\" + title) \n",
    "        content = eval('u\"' + comdata[j] + '\"')\n",
    "        print(\"内容：\" + content)\n",
    "    url = 'https://video.coral.qq.com/filmreviewr/c/upcomment/g7jpq0qd0k8xdca?callback=_filmreviewrcupcommentg7jpq0qd0k8xdca&reqnum=3&source=132&commentid=' + nextid + \"&_=1605598626157\"\n",
    "print(\"----结束爬虫----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节6-3 微信爬虫实战\n",
    "使用fiddler代理服务器爬取微信网页（https://weixin.sogou.com/ ）信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "\n",
    "# 自定义函数，功能为使用代理服务器爬一个网址\n",
    "def use_proxy(proxy_addr, url):\n",
    "    try:\n",
    "        req = urllib.request.Request(url)\n",
    "        req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36')\n",
    "        proxy = urllib.request.ProxyHandler({'http': proxy_addr})\n",
    "        opener = urllib.request.build_opener(proxy, urllib.request.HTTPHandler)\n",
    "        urllib.request.install_opener(opener)\n",
    "        data = urllib.request.urlopen(req).read()\n",
    "        return data\n",
    "    except urllib.error.URLError as e:\n",
    "        if hasattr(e, 'code'):\n",
    "            print(e.code)\n",
    "        if hasattr(e, 'reason'):\n",
    "            print(e.reason)\n",
    "        time.sleep(10)\n",
    "    except Exception as e:\n",
    "        print(\"Exception:\" + str(e))\n",
    "        time.sleep(1)\n",
    "\n",
    "key = \"python\"\n",
    "proxy_addr = '127.0.0.1:8888'\n",
    "for i in range(0, 10):\n",
    "    key = urllib.request.quote(key)\n",
    "    thispageurl = \"https://weixin.sogou.com/weixin?type=2&query={}&page={}\".format(key, str(i))\n",
    "    thispagedata = use_proxy(proxy, thispageurl)\n",
    "    print(len(str(thispagedata)))\n",
    "    pat1 = '<a href=\"(.*?)\"'\n",
    "    rs1 = re.compile(pat1, re.S).findall(str(thispagedata))\n",
    "    if(len(rs1) == 0):\n",
    "        print(\"此次{}页没爬取成功。\".format(str(i)))\n",
    "        continue\n",
    "    for j in range(0, len(rs1)):\n",
    "        thisurl = rs1[j]\n",
    "        thisurl = thisurl.replace(\"amp;\", \"\")\n",
    "        file = \"/result/第{}页第{}篇文章.html\".format(str(i), str(j))\n",
    "        thisdata = use_proxy(proxy, thisurl)\n",
    "        try:\n",
    "            fh = open(file, \"wb\")\n",
    "            fh.write(thisdata)\n",
    "            fh.close()\n",
    "            print(\"第{}页第{}篇文章成功\".format(str(i), str(j)))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"第{}页第{}篇文章失败\".format(str(i), str(j)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS.微信改版，旧的代码已经不可用，新的请参考。https://www.cnblogs.com/hyonline/p/11812977.html\n",
    "涉及反爬技术：Cookie构造和js加密"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "from urllib import parse\n",
    "\n",
    "\n",
    "def get_cookie(response1, uigs_para, UserAgent):\n",
    "    SetCookie = response1.headers['Set-Cookie']\n",
    "    cookie_params = {\n",
    "        \"ABTEST\": re.findall('ABTEST=(.*?);', SetCookie, re.S)[0],\n",
    "        \"SNUID\": re.findall('SNUID=(.*?);', SetCookie, re.S)[0],\n",
    "        \"IPLOC\": re.findall('IPLOC=(.*?);', SetCookie, re.S)[0],\n",
    "        \"SUID\": re.findall('SUID=(.*?);', SetCookie, re.S)[0]\n",
    "    }\n",
    "    \n",
    "    url = \"https://www.sogou.com/sug/css/m3.min.v.7.css\"\n",
    "    headers = {\n",
    "        \"Accept\": \"text/css,*/*;q=0.1\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Cookie\": \"SNUID={}; IPLOC={}\".format(cookie_params['SNUID'], cookie_params['IPLOC']),\n",
    "        \"Host\": \"www.sogou.com\",\n",
    "        \"Referer\": \"https://weixin.sogou.com/\",\n",
    "        \"User-Agent\": UserAgent\n",
    "    }\n",
    "    response2 = requests.get(url, headers=headers)\n",
    "    SetCookie = response2.headers['Set-Cookie']\n",
    "    cookie_params['SUID'] = re.findall('SUID=(.*?);', SetCookie, re.S)[0]\n",
    "    \n",
    "    url = \"https://weixin.sogou.com/websearch/wexinurlenc_sogou_profile.jsp\"\n",
    "    headers = {\n",
    "        \"Accept\": \"*/*\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Cookie\": \"ABTEST={}; SNUID={}; IPLOC={}; SUID={}\".format(cookie_params['ABTEST'], cookie_params['SNUID'], cookie_params['IPLOC'],\n",
    "                                                                  cookie_params['SUID']),\n",
    "        \"Host\": \"weixin.sogou.com\",\n",
    "        \"Referer\": response1.url,\n",
    "        \"User-Agent\": UserAgent\n",
    "    }\n",
    "    response3 = requests.get(url, headers=headers)\n",
    "    SetCookie = response3.headers['Set-Cookie']\n",
    "    cookie_params['JSESSIONID'] = re.findall('JSESSIONID=(.*?);', SetCookie, re.S)[0]\n",
    "    \n",
    "    url = \"https://pb.sogou.com/pv.gif\"\n",
    "    headers = {\n",
    "        \"Accept\": \"image/webp,*/*\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Cookie\": \"SNUID={}; IPLOC={}; SUID={}\".format(cookie_params['SNUID'], cookie_params['IPLOC'], cookie_params['SUID']),\n",
    "        \"Host\": \"pb.sogou.com\",\n",
    "        \"Referer\": \"https://weixin.sogou.com/\",\n",
    "        \"User-Agent\": UserAgent\n",
    "    }\n",
    "    response4 = requests.get(url, headers=headers, params=uigs_para)\n",
    "    SetCookie = response4.headers['Set-Cookie']\n",
    "    cookie_params['SUV'] = re.findall('SUV=(.*?);', SetCookie, re.S)[0]\n",
    "    \n",
    "    return cookie_params\n",
    "\n",
    "\n",
    "def get_k_h(url):\n",
    "    b = int(random.random() * 100) + 1\n",
    "    a = url.find(\"url=\")\n",
    "    url = url + \"&k=\" + str(b) + \"&h=\" + url[a + 4 + 21 + b: a + 4 + 21 + b + 1]\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_uigs_para(response):\n",
    "    uigs_para = re.findall('var uigs_para = (.*?);', response.text, re.S)[0]\n",
    "    if 'passportUserId ? \"1\" : \"0\"' in uigs_para:\n",
    "        uigs_para = uigs_para.replace('passportUserId ? \"1\" : \"0\"', '0')\n",
    "    uigs_para = json.loads(uigs_para)\n",
    "    exp_id = re.findall('uigs_para.exp_id = \"(.*?)\";', response.text, re.S)[0]\n",
    "    uigs_para['right'] = 'right0_0'\n",
    "    uigs_para['exp_id'] = exp_id[:-1]\n",
    "    return uigs_para\n",
    "\n",
    "\n",
    "def main_v4(list_url, UserAgent):\n",
    "    headers1 = {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"zh-CN,zh;q=0.9,en;q=0.8\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Host\": \"weixin.sogou.com\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"User-Agent\": UserAgent,\n",
    "    }\n",
    "    response1 = requests.get(list_url, headers=headers1)\n",
    "    html = etree.HTML(response1.text)\n",
    "    urls = ['https://weixin.sogou.com' + i for i in html.xpath('//div[@class=\"img-box\"]/a/@href')]\n",
    "    \n",
    "    uigs_para = get_uigs_para(response1)\n",
    "    params = get_cookie(response1, uigs_para, UserAgent)\n",
    "    approve_url = 'https://weixin.sogou.com/approve?uuid={}'.format(uigs_para['uuid'])\n",
    "    headers2 = {\n",
    "        \"Accept\": \"*/*\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Cookie\": \"ABTEST={}; IPLOC={}; SUID={}; SUV={}; SNUID={}; JSESSIONID={};\".format(params['ABTEST'], params['IPLOC'],\n",
    "                                                                                          params['SUID'], params['SUV'], params['SNUID'],\n",
    "                                                                                          params['JSESSIONID']),\n",
    "        \"Host\": \"weixin.sogou.com\",\n",
    "        \"Referer\": response1.url,\n",
    "        \"User-Agent\": UserAgent,\n",
    "        \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "    }\n",
    "    for url in urls:\n",
    "        response2 = requests.get(approve_url, headers=headers2)\n",
    "        url = get_k_h(url)\n",
    "        headers3 = {\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "            \"Accept-Language\": \"zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Cookie\": \"ABTEST={}; SNUID={}; IPLOC={}; SUID={}; JSESSIONID={}; SUV={}\".format(params['ABTEST'], params['SNUID'],\n",
    "                                                                                             params['IPLOC'], params['SUID'],\n",
    "                                                                                             params['JSESSIONID'],\n",
    "                                                                                             params['SUV']),\n",
    "            \"Host\": \"weixin.sogou.com\",\n",
    "            \"Referer\": list_url,\n",
    "            \"Upgrade-Insecure-Requests\": \"1\",\n",
    "            \"User-Agent\": UserAgent\n",
    "        }\n",
    "        response3 = requests.get(url, headers=headers3)\n",
    "        \n",
    "        fragments = re.findall(\"url \\+= '(.*?)'\", response3.text, re.S)\n",
    "        itemurl = ''\n",
    "        for i in fragments:\n",
    "            itemurl += i\n",
    "        \n",
    "        # 文章url拿正文\n",
    "        headers4 = {\n",
    "            \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3\",\n",
    "            \"accept-encoding\": \"gzip, deflate, br\",\n",
    "            \"accept-language\": \"zh-CN,zh;q=0.9,en;q=0.8\",\n",
    "            \"cache-control\": \"max-age=0\",\n",
    "            \"user-agent\": UserAgent\n",
    "        }\n",
    "        response4 = requests.get(itemurl, headers=headers4)\n",
    "        html = etree.HTML(response4.text)\n",
    "        print(response4.status_code)\n",
    "        print(html.xpath('//meta[@property=\"og:title\"]/@content')[0])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    key = \"咸蛋超人\"\n",
    "    url = 'https://weixin.sogou.com/weixin?type=2&s_from=input&query={}&_sug_=n&_sug_type_=&page=1'.format(parse.quote(key))\n",
    "    UserAgent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:70.0) Gecko/20100101 Firefox/70.0\"\n",
    "    main_v4(url, UserAgent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节6-4 多线程爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "import urllib.error\n",
    "\n",
    "headers = (\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36\")\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders=[headers]\n",
    "urllib.request.install_opener(opener)\n",
    "for i in range(1,3):\n",
    "    url = \"https://www.qiushibaike.com/8hr/page/\" + str(i)\n",
    "    pagedata = urllib.request.urlopen(url).read().decode(\"utf-8\", \"ignore\")\n",
    "    pat = '<div class=\"content\">.*?<span>(.*?)</span>.*?</div>'\n",
    "    datalist = re.compile(pat, re.S).findall(pagedata)\n",
    "    for j in range(0, len(datalist)):\n",
    "        print(\"第\" + str(i) + \"页第\" + str(j) + \"个段子的内容是：\")\n",
    "        print(datalist[j])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "class A(threading.Thread):\n",
    "    def __init__(self):\n",
    "        threading.Thread.__init__(self)\n",
    "    def run(self):\n",
    "        for i in range(10):\n",
    "            print(\"我是线程A\")\n",
    "class B(threading.Thread):\n",
    "    def __init__(self):\n",
    "        threading.Thread.__init__(self)\n",
    "    def run(self):\n",
    "        for i in range(10):\n",
    "            print(\"我是线程B\")\n",
    "t1 = A()\n",
    "t1.start()\n",
    "t2 = B()\n",
    "t2.start()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节6-5 Scrapy框架的安装\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# 章节8-1 天善智能课程自动爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "爬虫难点：1、反爬处理；2、抓包；3、分布式与多线程。\n",
    "验证码处理：1、图像识别；2、外部接口；3、半自动方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "link = 'http://www.santostang.com/'   # 定义link为目标网页地址\n",
    "# 定义请求头的浏览器代理，进行伪装。\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'}\n",
    "# r是requests的Response回复对象\n",
    "r = requests.get(link, headers = headers)\n",
    "# print(r.text)  # r.text是获取的网页内容代码\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")  # 使用BeautifulSoup解析\n",
    "title = soup.find(\"h1\", class_=\"post-title\").a.text.strip()\n",
    "print(title)\n",
    "html = etree.HTML(r.text)  # 使用xpath解析\n",
    "title = html.xpath('//*[@id=\"main\"]/div/div[1]/article[1]/header/h1/a/text()')[0]\n",
    "print(title)\n",
    "# 存储数据到txt\n",
    "with open('test.txt', 'a+') as f:\n",
    "    f.write(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get('http://www.santostang.com/')\n",
    "print('文本编码：' , r.encoding)\n",
    "print('响应状态码：' , r.status_code)\n",
    "print('JSON解码器：' , r.json)\n",
    "print('字节方式的响应体：' , r.content)\n",
    "print('字符串方式的响应体：' , r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "key_dict = {'key1': 'value1', 'key2': 'value2'}\n",
    "r = requests.get('http://httpbin.org/get', params=key_dict)\n",
    "print(\"URL已经正确编码：\", r.url)\n",
    "print(\"字符串方式的响应体：\\n\", r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "headers = {'Host': 'www.santostang.com', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'}\n",
    "r = requests.get('http://www.santostang.com', headers=headers)\n",
    "print('响应状态码：', r.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "key_dict = {'key1': 'value1', 'key2': 'value2'}\n",
    "r = requests.post('http://httpbin.org/post', data=key_dict)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "link = \"http://www.santostang.com/\"\n",
    "r = requests.get(link, timeout = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "\n",
    "def get_movies():\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'}\n",
    "    movie_list = []\n",
    "    for i in range(0, 10):\n",
    "        link = 'https://movie.douban.com/top250?start=' + str(i*25)\n",
    "        r = requests.get(link, headers=headers, timeout=20)\n",
    "        print(str(i+1), '页响应状态码：', r.status_code)\n",
    "        html = etree.HTML(r.text)\n",
    "        title_list = html.xpath('//div[@class=\"hd\"]/a/span[1]/text()')\n",
    "        for each in title_list:\n",
    "            movie_list.append(each)\n",
    "    return movie_list\n",
    "\n",
    "movies = get_movies()\n",
    "print(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def single_page_comment(link):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'}\n",
    "    r = requests.get(link, headers=headers)\n",
    "    json_string = r.text\n",
    "    json_string = json_string[json_string.find('{'):-2]\n",
    "    json_data = json.loads(json_string)\n",
    "    comment_list = json_data['results']['parents']\n",
    "    for each in comment_list:\n",
    "        message = each['content']\n",
    "        print(message)\n",
    "    \n",
    "for page in range(1, 4):\n",
    "    link1 = \"https://api-zero.livere.com/v1/comments/list?callback=jQuery112400642670146062918_1610003898881&limit=10&offset=\"\n",
    "    link2 = \"&repSeq=4272904&requestPath=%2Fv1%2Fcomments%2Flist&consumerSeq=1020&livereSeq=28583&smartloginSeq=5154&code=\"\n",
    "    link = link1 + str(page) + link2\n",
    "    print(\"第\"+str(page)+\"页: \", link)\n",
    "    single_page_comment(link)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "driver = webdriver.Firefox(executable_path = r'D:\\Program Files\\geckodriver\\geckodriver.exe')\n",
    "driver.implicitly_wait(20) # 隐性等待，最长等20秒\n",
    "driver.get(\"http://www.santostang.com/2018/07/04/hello-world/\")\n",
    "# time.sleep(5)\n",
    "# for i in range(0,3):\n",
    "#     driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") # 下滑到页面底部\n",
    "#     # 把所有评论加载出来，首先转换iframe，再找到查看更多，点击\n",
    "#     driver.switch_to.frame(driver.find_element_by_css_selector(\"iframe[title='livere-comment']\"))\n",
    "#     load_more = driver.find_element_by_css_selector('button.more-btn')\n",
    "#     load_more.click()\n",
    "#     # 把iframe又转回去\n",
    "#     driver.switch_to.default_content()\n",
    "#     time.sleep(2)\n",
    "\n",
    "driver.switch_to.frame(driver.find_element_by_css_selector(\"iframe[title='livere-comment']\"))\n",
    "comments = driver.find_elements_by_css_selector('div.reply-content')\n",
    "# print(comment)\n",
    "for eachcomment in comments:\n",
    "    content = eachcomment.find_element_by_tag_name('p')\n",
    "    print(content.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = dirver.find_element_by_name(\"username\")\n",
    "user.clear # 清除元素的内容、\n",
    "user.send_keys(\"123456\") # 模拟按键输入\n",
    "pwd = driver.find_element_by_name(\"password\") # 找到密码输入框\n",
    "pwd.clear\n",
    "pwd.send_keys(\"123456\")\n",
    "dirver.find_element_by_id(\"loginBtn\").click() # 单击登录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//*[@id=\"listing-46887483\"]/div[2]/div[2]/div/div/div[1]/div/span/span/span/span/span[2]\n",
    "https://www.airbnb.cn/s/Shenzhen--China/homes?refinement_paths%5B%5D=%2Fhomes&current_tab_id=home_tab&selected_tab_id=home_tab&screen_size=large&hide_dates_and_guests_filters=false&place_id=ChIJkVLh0Aj0AzQRyYCStw1V7v0&map_toggle=false\n",
    "https://www.airbnb.cn/s/Shenzhen--China/homes?refinement_paths%5B%5D=%2Fhomes&current_tab_id=home_tab&selected_tab_id=home_tab&screen_size=large&hide_dates_and_guests_filters=false&map_toggle=false&place_id=ChIJkVLh0Aj0AzQRyYCStw1V7v0&s_tag=LBUutDen&last_search_session_id=b293d63f-e3fe-4543-8bea-509eff66f7b1&items_offset=20&section_offset=6\n",
    "https://www.airbnb.cn/s/Shenzhen--China/homes?refinement_paths%5B%5D=%2Fhomes&current_tab_id=home_tab&selected_tab_id=home_tab&screen_size=medium&hide_dates_and_guests_filters=false&map_toggle=false&s_tag=LBUutDen&place_id=ChIJkVLh0Aj0AzQRyYCStw1V7v0&last_search_session_id=ddabba5b-96eb-4aa1-b863-255c8759097f&items_offset=40&section_offset=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "driver = webdriver.Firefox(executable_path = r'D:\\Program Files\\geckodriver\\geckodriver.exe')\n",
    "#把上述地址改成你电脑中geckodriver.exe程序的地址\n",
    "#在虚拟浏览器中打开 Airbnb 页面\n",
    "driver.get(\"https://zh.airbnb.com/s/Shenzhen--China/homes\")\n",
    "\n",
    "#找到页面中所有的出租房\n",
    "rent_list = driver.find_elements_by_css_selector('div._gig1e7')\n",
    "\n",
    "#对于每一个出租房\n",
    "for eachhouse in rent_list:\n",
    "    #找到评论数量\n",
    "    try:\n",
    "        comment = eachhouse.find_element_by_css_selector('span._69pvqtq')\n",
    "        comment = comment.text\n",
    "    except:\n",
    "        comment = 0\n",
    "    \n",
    "    #找到价格\n",
    "#     price = eachhouse.find_element_by_css_selector('span._1d8yint7')\n",
    "#     price = price.text.replace(\"每晚\", \"\").replace(\"价格\", \"\").replace(\"\\n\", \"\")\n",
    "    \n",
    "    #找到名称\n",
    "    name = eachhouse.find_element_by_css_selector('div._qrfr9x5')\n",
    "    name = name.text\n",
    "    \n",
    "    #找到房屋类型，大小\n",
    "    details = eachhouse.find_element_by_css_selector('span._faldii7')\n",
    "    details = details.text\n",
    "    house_type = details.split(\" · \")[0]\n",
    "    bed_number = details.split(\" · \")[1]\n",
    "    print (comment, name, house_type, bed_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "driver = webdriver.Firefox(executable_path = r'D:\\Program Files\\geckodriver\\geckodriver.exe')\n",
    "#把上述地址改成你电脑中geckodriver.exe程序的地址\n",
    "for i in range(0,2):\n",
    "    link = \"https://zh.airbnb.com/s/Shenzhen--China/homes?items_offset=\" + str(i * 20)\n",
    "    driver.get(link)\n",
    "    rent_list = driver.find_elements_by_css_selector('div._gig1e7')\n",
    "\n",
    "    for eachhouse in rent_list:\n",
    "        try:\n",
    "            comment = eachhouse.find_element_by_css_selector('span._1clmxfj').text\n",
    "        except:\n",
    "            comment = '缺失'\n",
    "        try:\n",
    "            price = eachhouse.find_element_by_css_selector('div._1ixtnfc')\n",
    "            price = price.text.replace(\"每晚\", \"\").replace(\"价格\", \"\").replace(\"\\n\", \"\")\n",
    "        except:\n",
    "            price = '缺失'\n",
    "        try:\n",
    "            name = eachhouse.find_element_by_css_selector('div._qrfr9x5')\n",
    "            name = name.text\n",
    "        except:\n",
    "            name = '缺失'\n",
    "        try:\n",
    "            details = eachhouse.find_element_by_css_selector('span._faldii7')\n",
    "            details = details.text\n",
    "            house_type = details.split(\" · \")[0]\n",
    "            bed_number = details.split(\" · \")[1]\n",
    "        except:\n",
    "            house_type, bed_number = '缺失', '缺失'\n",
    "        print (comment, price, name, house_type, bed_number)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = '[0-9]+'\n",
    "line = 'The first number is 12345, 12345 is the second.'\n",
    "print(re.match(pattern, line))\n",
    "print(re.search(pattern, line))\n",
    "print(re.findall(pattern, line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.98 Safari/537.36'}\n",
    "for i in range(1,11):\n",
    "    link = 'https://beijing.anjuke.com/sale/p' + str(i)\n",
    "    r = requests.get(link, headers = headers)\n",
    "    print ('现在爬取的是第', i, '页')\n",
    "\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    house_list = soup.find_all('li', class_=\"list-item\")\n",
    "\n",
    "    for house in house_list:\n",
    "        try:\n",
    "            name = house.find('div', class_ ='house-title').a.text.strip()\n",
    "            price = house.find('span', class_='price-det').text.strip()\n",
    "            price_area = house.find('span', class_='unit-price').text.strip()\n",
    "\n",
    "            no_room = house.find('div', class_='details-item').span.text\n",
    "            area = house.find('div', class_='details-item').contents[3].text\n",
    "            floor = house.find('div', class_='details-item').contents[5].text\n",
    "            year = house.find('div', class_='details-item').contents[7].text\n",
    "            broker = house.find('span', class_='broker-name broker-text').text\n",
    "            broker = broker[1:]\n",
    "            address = house.find('span', class_='comm-address').text.strip()\n",
    "            address = address.replace('\\xa0\\xa0\\n                    ','  ')\n",
    "            tag_list = house.find_all('span', class_='item-tags')\n",
    "            tags = [i.text for i in tag_list] \n",
    "            print (name, price, price_area, no_room, area, floor, year, broker, address, tags)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "# 打开数据库连接\n",
    "conn = pymysql.connect(host=\"127.0.0.1\", user=\"root\", password=\"root123456.\", db=\"dangdang\")\n",
    "# SQL操作\n",
    "sql = \"INSERT INTO books(title, link, cost, bookshop, comment) VALUES(%s, %s, %s, %s, %s)\"\n",
    "param = (\"test2\", \"test2\", \"test\", \"test\", \"test\")\n",
    "try:\n",
    "    # 使用cursor()方法获取操作游标\n",
    "    cursor = conn.cursor()\n",
    "    # 执行sql语句\n",
    "    cursor.execute(sql, param)\n",
    "    # 提交到数据库执行\n",
    "    conn.commit()\n",
    "    print(\"插入成功！\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    # 如果发生错误则回滚\n",
    "    conn.rollback()\n",
    "finally:\n",
    "    # 关闭数据库连接\n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.blog_database\n",
    "collection = db.blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.blog_database\n",
    "collection = db.blog\n",
    "link = \"http://www.santostang.com/\"\n",
    "headers = {'User-Agent' : 'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'} \n",
    "r = requests.get(link, headers= headers)\n",
    "soup = BeautifulSoup(r.text, \"lxml\")\n",
    "title_list = soup.find_all(\"h1\", class_=\"post-title\")\n",
    "for eachone in title_list:\n",
    "    url = eachone.a['href']\n",
    "    title = eachone.a.text.strip()\n",
    "    post = {\"url\": url, \"title\": title, \"data\": datetime.datetime.utcnow()}\n",
    "    collection.insert_one(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# 获取页面\n",
    "def get_page(link):\n",
    "    headers = {'User-Agent' : 'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'} \n",
    "    r = requests.get(link, headers = headers)\n",
    "    html = r.content  #使用r.content解封装\n",
    "    html = html.decode('utf-8')  #由UTF-8解码为unicode\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    return soup\n",
    "\n",
    "# 解析网页\n",
    "def get_data(post_list):\n",
    "    data_list =[]\n",
    "    for post in post_list:\n",
    "        title = post.find('div',class_='titlelink box').text.strip()\n",
    "        post_link = post.find('div',class_='titlelink box').a['href']\n",
    "        post_link = \"https://bbs.hupu.com\" + post_link \n",
    "\n",
    "        author = post.find('div',class_='author box').a.text.strip()\n",
    "        author_page = post.find('div',class_='author box').a['href']\n",
    "        start_date = post.find('div',class_='author box').contents[5].text.strip()\n",
    "\n",
    "        reply_view = post.find('span',class_='ansour box').text.strip()\n",
    "        reply = reply_view.split('/')[0].strip()\n",
    "        view = reply_view.split('/')[1].strip()\n",
    "\n",
    "        reply_time = post.find('div',class_='endreply box').a.text.strip()\n",
    "        last_reply = post.find('div',class_='endreply box').span.text.strip()\n",
    "        if ':' in reply_time: #时间是11:27\n",
    "            date_time = str(datetime.date.today()) + ' ' + reply_time\n",
    "            date_time = datetime.datetime.strptime(date_time, '%Y-%m-%d %H:%M')\n",
    "        elif reply_time.find(\"-\") == 4: #时间是2017-02-27\n",
    "            date_time = datetime.datetime.strptime(reply_time, '%Y-%m-%d').date()\n",
    "        else: #时间是11-27\n",
    "            date_time = datetime.datetime.strptime('2018-' + reply_time, '%Y-%m-%d').date()\n",
    "        data_list.append([title, post_link, author, author_page, start_date, reply, view, last_reply, date_time])\n",
    "    return data_list\n",
    "\n",
    "class MongoAPI(object):\n",
    "    def __init__(self, db_ip, db_port, db_name, table_name):\n",
    "        self.db_ip = db_ip\n",
    "        self.db_port = db_port\n",
    "        self.db_name = db_name\n",
    "        self.table_name = table_name\n",
    "        self.conn = MongoClient(host=self.db_ip, port=self.db_port)\n",
    "        self.db = self.conn[self.db_name]\n",
    "        self.table = self.db[self.table_name]\n",
    "    def get_one(self, query):\n",
    "        return self.table.find_one(query, projection={\"_id\": False})\n",
    "    def get_all(self, query):\n",
    "        return self.table.find(query)\n",
    "    def add(self, kv_dict):\n",
    "        return self.table.insert_one(kv_dict)\n",
    "    def delete(self, query):\n",
    "        return self.table.delete_many(query)\n",
    "    def check_exist(self, query):\n",
    "        ret = self.table.find_one(query)\n",
    "        return ret != None\n",
    "    # 如果没有会新建\n",
    "    def update(self, query, kv_dict):\n",
    "            self.table.update_one(query,{\n",
    "              '$set': kv_dict\n",
    "            }, upsert=True)\n",
    "\n",
    "link = \"https://bbs.hupu.com/bxj\"\n",
    "soup = get_page(link)\n",
    "post_all= soup.find('ul', class_=\"for-list\")\n",
    "post_list = post_all.find_all('li')\n",
    "data_list = get_data(post_list)\n",
    "for each in data_list:\n",
    "    print (each)\n",
    "    \n",
    "hupu_post = MongoAPI(\"localhost\",  27017,  \"hupu\", \"post\")\n",
    "for each in data_list:\n",
    "    hupu_post.update({\"post_link\": each[1]},{\"title\": each[0], \n",
    "                    \"post_link\": each[1],\n",
    "                   \"author\": each[2],\n",
    "                   \"author_page\": each[3],\n",
    "                   \"start_date\": str(each[4]),\n",
    "                   \"reply\": each[5],\n",
    "                   \"view\": each[6],\n",
    "                   \"last_reply\": each[7],\n",
    "                   \"last_reply_time\": str(each[8])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "output_list = ['1', '2', '3', '4']\n",
    "with open ('test.csv', 'a+', encoding='UTF-8', newline='') as csvfile:\n",
    "    w = csv.writer(csvfile)\n",
    "    w.writerow(output_list)\n",
    "    \n",
    "with open('test.csv', 'r', encoding='UTF-8') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    for row in csv_reader:\n",
    "        print(row)\n",
    "        print(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "\n",
    "link = \"http://www.santostang.com/\"\n",
    "\n",
    "def scrap(link):\n",
    "    headers = {'User-Agent' : 'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'} \n",
    "    r = requests.get(link, headers= headers)\n",
    "    html = r.text\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    return soup\n",
    "\n",
    "soup = scrap(link)\n",
    "title_list = soup.find_all(\"h1\", class_=\"post-title\")\n",
    "scrap_times = 0\n",
    "for eachone in title_list:\n",
    "    url = eachone.a['href']\n",
    "    print ('开始爬取这篇博客: ', url)\n",
    "    soup_article = scrap(url)\n",
    "    title = soup_article.find(\"h1\", class_=\"view-title\").text.strip()\n",
    "    print ('这篇博客的标题为: ', title)\n",
    "    \n",
    "    scrap_times += 1\n",
    "    if scrap_times % 5 == 0:\n",
    "        sleep_time = 10 + random.random()\n",
    "    else:\n",
    "        sleep_time = random.randint(0,2) + random.random()\n",
    "    time.sleep(sleep_time)\n",
    "    print ('开始休息: ', sleep_time, '秒')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap_times = 0\n",
    "for eachone in title_list:\n",
    "    url = eachone.a['href']\n",
    "    print ('开始爬取这篇博客: ', url)\n",
    "    soup_article = scrap(url)\n",
    "    title = soup_article.find(\"h1\", class_=\"view-title\").text.strip()\n",
    "    print ('这篇博客的标题为: ', title)\n",
    "    \n",
    "    scrap_times += 1\n",
    "    if scrap_times % 5 == 0:\n",
    "        sleep_time = 10 + random.random()\n",
    "    else:\n",
    "        sleep_time = random.randint(0,2) + random.random()\n",
    "    time.sleep(sleep_time)\n",
    "    print ('开始休息: ', sleep_time, '秒')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<!-- [ published at 2021-01-14 11:48:01 ] -->\n",
      "<html>\n",
      "<head>\n",
      "    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n",
      "    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\" />\n",
      "    <title>æ°æµªé¦é¡µ</title>\n",
      "\t<meta name=\"keywords\" content=\"æ°æµª,æ°æµªç½,SINA,sina,sina.com.cn,æ°æµªé¦é¡µ,é¨æ·,èµè®¯\" />\n",
      "\t<meta name=\"description\" content=\"æ°æµªç½ä¸ºå",
      "¨çç¨æ·24å°æ¶æä¾å",
      "¨é¢åæ¶çä¸­æèµè®¯ï¼å",
      "å®¹è¦çå½å",
      "å¤çªåæ°é»äºä»¶ãä½åèµäºãå¨±ä¹æ¶å°ãäº§ä¸èµè®¯ãå®ç¨ä¿¡æ¯ç­ï¼è®¾ææ°é»ãä½è²ãå¨±ä¹ãè´¢ç»ãç§æãæ¿äº§ãæ±½è½¦ç­30å¤ä¸ªå",
      "å®¹é¢éï¼åæ¶å¼è®¾åå®¢ãè§é¢ãè®ºåç­èªç±äºå¨äº¤æµç©ºé´ã\" />\n",
      "\t<meta content=\"always\" name=\"referrer\">\n",
      "    <link rel=\"mask-icon\" sizes=\"any\" href=\"//www.sina.com.cn/favicon.svg\" color=\"red\">\n",
      "\t<meta name=\"stencil\" content=\"PGLS000022\" />\n",
      "\t<meta name=\"publishid\" content=\"30,131,1\" />\n",
      "\t<meta name=\"verify-v1\" content=\"6HtwmypggdgP1NLw7NOuQBI2TW8+Cfk\n",
      "解压后字符串的编码为 {'encoding': 'utf-8', 'confidence': 0.99, 'language': ''}\n",
      "<!DOCTYPE html>\n",
      "<!-- [ published at 2021-01-14 11:48:01 ] -->\n",
      "<html>\n",
      "<head>\n",
      "    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n",
      "    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\" />\n",
      "    <title>新浪首页</title>\n",
      "\t<meta name=\"keywords\" content=\"新浪,新浪网,SINA,sina,sina.com.cn,新浪首页,门户,资讯\" />\n",
      "\t<meta name=\"description\" content=\"新浪网为全球用户24小时提供全面及时的中文资讯，内容覆盖国内外突发新闻事件、体坛赛事、娱乐时尚、产业资讯、实用信息等，设有新闻、体育、娱乐、财经、科技、房产、汽车等30多个内容频道，同时开设博客、视频、论坛等自由互动交流空间。\" />\n",
      "\t<meta content=\"always\" name=\"referrer\">\n",
      "    <link rel=\"mask-icon\" sizes=\"any\" href=\"//www.sina.com.cn/favicon.svg\" color=\"red\">\n",
      "\t<meta name=\"stencil\" content=\"PGLS000022\" />\n",
      "\t<meta name=\"publishid\" content=\"30,131,1\" />\n",
      "\t<meta name=\"verify-v1\" content=\"6HtwmypggdgP1NLw7NOuQBI2TW8+CfkYCoyeB8IDbn8=\" />\n",
      "\t<meta name=\"application-name\" content=\"新浪首页\"/>\n",
      "\t<meta name =\"msapplication-TileImage\" content=\"//i1.sinaimg.cn/dy/deco/2013/0312/logo.png\"/>\n",
      "\t<meta name=\"msapplication-TileColor\" content=\"#ffbf27\"/>\n",
      "<link rel=\"apple-touch-icon\" href=\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import chardet\n",
    "\n",
    "url = 'http://www.sina.com.cn/'\n",
    "r = requests.get(url)\n",
    "print(r.text[:1000])\n",
    "\n",
    "after_gzip = r.content\n",
    "print('解压后字符串的编码为',chardet.detect(after_gzip))\n",
    "print(after_gzip.decode('utf-8')[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
