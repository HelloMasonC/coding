{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# 章节3-1 使用Urllib\n",
    "爬取CSDN（网址：https://edu.csdn.net/course/detail/29493 ）的一个课程页，并自动提取出QQ群。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "data = urllib.request.urlopen(\"https://edu.csdn.net/course/detail/29493\").read().decode('utf8')\n",
    "pat = '<span class=\"realname\" data-v-a39d224e>(.*?)</span>'\n",
    "result = re.compile(pat).findall(str(data))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节4-1 timeout设置\n",
    "循环爬取首页（网址:https://edu.hellobi.com/ ），0.5秒无响应超时异常。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# urllib.request.urlretrieve(\"https://edu.hellobi.com/\", filename=\"1.html\")\n",
    "# urllib.request.urlcleanup()\n",
    "for i in range(10):\n",
    "    try:\n",
    "        file = urllib.request.urlopen(\"https://edu.hellobi.com/\", timeout = 0.5)\n",
    "        print(file.getcode())\n",
    "    except Exception as e:\n",
    "        print(\"访问{0}出现异常：{1}\".format(file.geturl(), str(e)))\n",
    "# file.geturl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节4-2 网址有中文\n",
    "爬取百度（网址：http://www.baidu.com/ ）查找“Python”和“计算机”的查询结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "keywd1 = \"python\"\n",
    "keywd2 = urllib.request.quote(\"计算机\")\n",
    "url1 = \"http://www.baidu.com/s?wd=\" + keywd1\n",
    "url2 = \"http://www.baidu.com/s?wd=\" + keywd2\n",
    "req1 = urllib.request.Request(url1)\n",
    "data1 = urllib.request.urlopen(req1).read()\n",
    "req2= urllib.request.Request(url2)\n",
    "data2 = urllib.request.urlopen(req2).read()\n",
    "with open(\"1.html\", \"wb\") as f:\n",
    "    f.write(data1)\n",
    "with open(\"2.html\", \"wb\") as f:\n",
    "    f.write(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节4-3 使用post\n",
    "向网页（网址：http://www.iqianyue.com/mypost/ ）提交。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "url = \"http://www.iqianyue.com/mypost/\"\n",
    "mydata = urllib.parse.urlencode({\"name\":\"test@test.com\", \"pass\":\"123456jkl\"}).encode(\"utf-8\")\n",
    "req = urllib.request.Request(url, mydata)\n",
    "print(data, req)\n",
    "result = urllib.request.urlopen(req).read()\n",
    "print(result.decode(\"utf-8\"))\n",
    "# with open(\"1.html\", \"wb\") as fl:\n",
    "#     fl.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# 章节4-4 异常处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.error\n",
    "import urllib.request\n",
    "\n",
    "try:\n",
    "    result = urllib.request.urlopen(\"http://blog.csdn.net/ss12rwew\")\n",
    "except urllib.error.URLError as e:\n",
    "    if hasattr(e, \"code\"):\n",
    "        print(e.code)\n",
    "    if hasattr(e, \"reason\"):\n",
    "        print(e.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节4-5 使用urllib.request.urlretrieve\n",
    "爬取新浪新闻（网址：https://news.sina.com.cn/ ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "data = urllib.request.urlopen(\"https://news.sina.com.cn/\").read()\n",
    "data2 = data.decode(\"utf-8\", \"ignore\")\n",
    "pat = 'href=\"(https://news.sina.com.cn/.*?)\"'\n",
    "allurl = re.compile(pat).findall(data2)\n",
    "# print(allurl)\n",
    "for i in range(len(allurl)):\n",
    "    try:\n",
    "        print(\"第{}次爬取\".format(i+1))\n",
    "        file = \"sinanews/\" + str(i+1) + \".html\"\n",
    "        urllib.request.urlretrieve(allurl[i], file)\n",
    "        print(\"----成功----\")\n",
    "    except urllib.error.HTTPError as e:\n",
    "        print(str(e.code) + \": \" + e.reason)\n",
    "    except Exception as e:\n",
    "        print(\"失败：{}\".format(str(e)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# 章节5-1 爬虫防屏蔽手段-代理服务器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "url = \"http://blog.csdn.net/\"\n",
    "headers = (\"user-agent\", \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36\")\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [headers]\n",
    "urllib.request.install_opener(opener)\n",
    "data = urllib.request.urlopen(url).read().decode(\"utf-8\", \"ignore\")\n",
    "print(\"output: \" + data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "def use_proxy(url, proxy_addr):\n",
    "    proxy = urllib.request.ProxyHandler({\"http\": proxy_addr})\n",
    "    opener = urllib.request.build_opener(proxy, urllib.request.HTTPHandler)\n",
    "    urllib.request.install_opener(opener)\n",
    "    data = urllib.request.urlopen(url).read().decode(\"utf-8\", \"ignore\")\n",
    "    return data\n",
    "\n",
    "proxy_addr = \"47.107.160.99:8118\"\n",
    "url = \"http://www.baidu.com\"\n",
    "data = use_proxy(url, proxy_addr)\n",
    "print(\"output: \" + data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "爬取快代理免费代理IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import random\n",
    "from lxml import etree\n",
    "from fake_useragent import UserAgent\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "def get_ip():\n",
    "    while True:\n",
    "        if not q.empty():\n",
    "        \t# 验证IP是否可用网址\n",
    "            url = 'http://httpbin.org/get'\n",
    "            proxies = q.get()\n",
    "            try:\n",
    "                html = requests.get(url, headers=headers, proxies=proxies, timeout=5).text\n",
    "                print('ip可以用')\n",
    "                with open('ip.txt','a')as f:\n",
    "                    f.write(str(proxies))\n",
    "                    f.write('\\n')\n",
    "            except:\n",
    "                print('ip不可用，下一个\\t')\n",
    "        else:\n",
    "            break\n",
    "\n",
    "def main():\n",
    "    t_list = []\n",
    "    for i in range(5):\n",
    "        t = Thread(target=get_ip)\n",
    "        t_list.append(t)\n",
    "        t.start()\n",
    "\n",
    "    for t in t_list:\n",
    "        t.join()\n",
    "\n",
    "\n",
    "ip_list = []\n",
    "q = Queue()\n",
    "for i in range(1, 10):\n",
    "    url = 'https://www.kuaidaili.com/free/inha/{}'.format(i)\n",
    "    print(url)\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36',\n",
    "    }\n",
    "    # proxies = {'http': 'http://211.159.219.225:8118', 'https': 'https://211.159.219.225:8118'}\n",
    "    # html = requests.get(url, headers=headers,proxies=proxies).text\n",
    "    html = requests.get(url, headers=headers).text\n",
    "    # print(html)\n",
    "    parse_html = etree.HTML(html)\n",
    "    tr_list = parse_html.xpath('//*[@id=\"list\"]/table/tbody/tr')\n",
    "    # 延迟访问6到11秒。\n",
    "    sleep = random.randint(6, 11)\n",
    "    print(f'等待{sleep}秒')\n",
    "    time.sleep(sleep)\n",
    "    print('开始')\n",
    "    for tr in tr_list[1:]:\n",
    "        ip = tr.xpath('./td[1]/text()')[0]\n",
    "        port = tr.xpath('./td[2]/text()')[0]\n",
    "        proxies = {\n",
    "            'http': f'http://{ip}:{port}',\n",
    "            'https': f'https://{ip}:{port}',\n",
    "        }\n",
    "        print(proxies)\n",
    "        # 存入队列\n",
    "        q.put(proxies)\n",
    "    main()\n",
    "\n",
    "print(ip_list)\n",
    "print(\"----结束----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# 章节5-2 图片爬虫\n",
    "把千图网（https://www.58pic.com/ ）某个频道的所有图片爬下来，高清原版的。(网址变了)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "headers = (\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36\")\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders=[headers]\n",
    "urllib.request.install_opener(opener)\n",
    "\n",
    "for i in range(1,2):\n",
    "#     pageurl = \"https://www.58pic.com/haibaomoban/0/id-{}.html\".format(str(i*25))\n",
    "    pageurl = \"https://www.58pic.com/tupian/so-0-0-default-0-0-SO-0_10_0_0_0_0_0-0-{}.html\".format(str(i))\n",
    "    data = urllib.request.urlopen(pageurl).read().decode(\"utf-8\", \"ignore\")\n",
    "    pat = 'data-original=\"//(preview.qiantucdn.com/.*?new_nowater)\"'\n",
    "    imglist = re.compile(pat).findall(data)\n",
    "    for j in range(0, len(imglist)):\n",
    "        try:\n",
    "            thisimg = imglist[j]\n",
    "#             thisimgurl = thisimg + \"_1024.jpg\"\n",
    "            thisimgurl = \"http://\" + thisimg\n",
    "            file = \"img/\" + str(i) + str(j) + \".jpg\"\n",
    "            urllib.request.urlretrieve(thisimgurl, filename = file)\n",
    "            print(\"----爬取第{}页第{}个图片成功----\".format(str(i+1), str(j+1)))\n",
    "        except urllib.error.URLError as e:\n",
    "            if hasattr(e, \"code\"):\n",
    "                print(e.code)\n",
    "            if hasattr(e, \"reason\"):\n",
    "                print(e.reason)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "爬取豆瓣电影Top250（https://movie.douban.com/top250 ）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "# keyname=\"短裤\"\n",
    "# key = urllib.request.quote(keyname)\n",
    "headers = (\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36\")\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders=[headers]\n",
    "urllib.request.install_opener(opener)\n",
    "print(\"----开始爬取----\")\n",
    "for i in range(0, 10):\n",
    "    url = \"https://movie.douban.com/top250?start={0}&filter=\".format(str(i*25))\n",
    "    data = urllib.request.urlopen(url).read().decode(\"utf-8\", \"ignore\")\n",
    "    pat = 'src=\"(https://img.*?)\" class=\"\"'\n",
    "    imageurl_list = re.compile(pat).findall(data)\n",
    "    for j in range(len(imageurl_list)):\n",
    "        thisimg = imageurl_list[j]\n",
    "        file = \"img/\" + str(i) + str(j) + \".jpg\"\n",
    "        urllib.request.urlretrieve(thisimg, filename = file)\n",
    "    print(\"----爬取第{}页成功----\".format(str(i+1)))\n",
    "\n",
    "print(\"爬取网页列表为：\" + str(imageurl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# 章节6-1 抓包分析\n",
    "爬取腾讯视频（青云志）前20条评论（深度解读）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "headers = (\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36\")\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders=[headers]\n",
    "urllib.request.install_opener(opener)\n",
    "comid, total = \"\", 0\n",
    "url = 'https://video.coral.qq.com/filmreviewr/c/upcomment/g7jpq0qd0k8xdca?callback=_filmreviewrcupcommentg7jpq0qd0k8xdca&reqnum=3&source=132&commentid=' + comid + \"&_=1605598626157\"\n",
    "while total <= 20:\n",
    "    try:\n",
    "        data = urllib.request.urlopen(url, timeout = 10).read().decode()\n",
    "    except urllib.error.URLError as e:\n",
    "        if hasattr(e, \"code\"):\n",
    "            print(e.code)\n",
    "        if hasattr(e, \"reason\"):\n",
    "            print(e.reason)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    patnext = '\"last\":\"(.*?)\"'\n",
    "    nextid = re.compile(patnext).findall(data)[0]\n",
    "    pattitle = '\"title\":\"(.*?)\",'\n",
    "    comtitle = re.compile(pattitle).findall(data)\n",
    "    patcom = '\"content\":\"(.*?)\",'\n",
    "    comdata = re.compile(patcom).findall(data)\n",
    "    for j in range(len(comdata)):\n",
    "        total += 1\n",
    "        if total > 20:\n",
    "            break\n",
    "        print(\"----第{0}条评论----\".format(str(total)))\n",
    "        title = eval('u\"' + comtitle[j] + '\"') \n",
    "        print(\"标题：\" + title) \n",
    "        content = eval('u\"' + comdata[j] + '\"')\n",
    "        print(\"内容：\" + content)\n",
    "    url = 'https://video.coral.qq.com/filmreviewr/c/upcomment/g7jpq0qd0k8xdca?callback=_filmreviewrcupcommentg7jpq0qd0k8xdca&reqnum=3&source=132&commentid=' + nextid + \"&_=1605598626157\"\n",
    "print(\"----结束爬虫----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节6-3 微信爬虫实战\n",
    "使用fiddler代理服务器爬取微信网页（https://weixin.sogou.com/ ）信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "\n",
    "# 自定义函数，功能为使用代理服务器爬一个网址\n",
    "def use_proxy(proxy_addr, url):\n",
    "    try:\n",
    "        req = urllib.request.Request(url)\n",
    "        req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36')\n",
    "        proxy = urllib.request.ProxyHandler({'http': proxy_addr})\n",
    "        opener = urllib.request.build_opener(proxy, urllib.request.HTTPHandler)\n",
    "        urllib.request.install_opener(opener)\n",
    "        data = urllib.request.urlopen(req).read()\n",
    "        return data\n",
    "    except urllib.error.URLError as e:\n",
    "        if hasattr(e, 'code'):\n",
    "            print(e.code)\n",
    "        if hasattr(e, 'reason'):\n",
    "            print(e.reason)\n",
    "        time.sleep(10)\n",
    "    except Exception as e:\n",
    "        print(\"Exception:\" + str(e))\n",
    "        time.sleep(1)\n",
    "\n",
    "key = \"python\"\n",
    "proxy_addr = '127.0.0.1:8888'\n",
    "for i in range(0, 10):\n",
    "    key = urllib.request.quote(key)\n",
    "    thispageurl = \"https://weixin.sogou.com/weixin?type=2&query={}&page={}\".format(key, str(i))\n",
    "    thispagedata = use_proxy(proxy, thispageurl)\n",
    "    print(len(str(thispagedata)))\n",
    "    pat1 = '<a href=\"(.*?)\"'\n",
    "    rs1 = re.compile(pat1, re.S).findall(str(thispagedata))\n",
    "    if(len(rs1) == 0):\n",
    "        print(\"此次{}页没爬取成功。\".format(str(i)))\n",
    "        continue\n",
    "    for j in range(0, len(rs1)):\n",
    "        thisurl = rs1[j]\n",
    "        thisurl = thisurl.replace(\"amp;\", \"\")\n",
    "        file = \"/result/第{}页第{}篇文章.html\".format(str(i), str(j))\n",
    "        thisdata = use_proxy(proxy, thisurl)\n",
    "        try:\n",
    "            fh = open(file, \"wb\")\n",
    "            fh.write(thisdata)\n",
    "            fh.close()\n",
    "            print(\"第{}页第{}篇文章成功\".format(str(i), str(j)))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"第{}页第{}篇文章失败\".format(str(i), str(j)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS.微信改版，旧的代码已经不可用，新的请参考。https://www.cnblogs.com/hyonline/p/11812977.html\n",
    "涉及反爬技术：Cookie构造和js加密"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "from urllib import parse\n",
    "\n",
    "\n",
    "def get_cookie(response1, uigs_para, UserAgent):\n",
    "    SetCookie = response1.headers['Set-Cookie']\n",
    "    cookie_params = {\n",
    "        \"ABTEST\": re.findall('ABTEST=(.*?);', SetCookie, re.S)[0],\n",
    "        \"SNUID\": re.findall('SNUID=(.*?);', SetCookie, re.S)[0],\n",
    "        \"IPLOC\": re.findall('IPLOC=(.*?);', SetCookie, re.S)[0],\n",
    "        \"SUID\": re.findall('SUID=(.*?);', SetCookie, re.S)[0]\n",
    "    }\n",
    "    \n",
    "    url = \"https://www.sogou.com/sug/css/m3.min.v.7.css\"\n",
    "    headers = {\n",
    "        \"Accept\": \"text/css,*/*;q=0.1\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Cookie\": \"SNUID={}; IPLOC={}\".format(cookie_params['SNUID'], cookie_params['IPLOC']),\n",
    "        \"Host\": \"www.sogou.com\",\n",
    "        \"Referer\": \"https://weixin.sogou.com/\",\n",
    "        \"User-Agent\": UserAgent\n",
    "    }\n",
    "    response2 = requests.get(url, headers=headers)\n",
    "    SetCookie = response2.headers['Set-Cookie']\n",
    "    cookie_params['SUID'] = re.findall('SUID=(.*?);', SetCookie, re.S)[0]\n",
    "    \n",
    "    url = \"https://weixin.sogou.com/websearch/wexinurlenc_sogou_profile.jsp\"\n",
    "    headers = {\n",
    "        \"Accept\": \"*/*\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Cookie\": \"ABTEST={}; SNUID={}; IPLOC={}; SUID={}\".format(cookie_params['ABTEST'], cookie_params['SNUID'], cookie_params['IPLOC'],\n",
    "                                                                  cookie_params['SUID']),\n",
    "        \"Host\": \"weixin.sogou.com\",\n",
    "        \"Referer\": response1.url,\n",
    "        \"User-Agent\": UserAgent\n",
    "    }\n",
    "    response3 = requests.get(url, headers=headers)\n",
    "    SetCookie = response3.headers['Set-Cookie']\n",
    "    cookie_params['JSESSIONID'] = re.findall('JSESSIONID=(.*?);', SetCookie, re.S)[0]\n",
    "    \n",
    "    url = \"https://pb.sogou.com/pv.gif\"\n",
    "    headers = {\n",
    "        \"Accept\": \"image/webp,*/*\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Cookie\": \"SNUID={}; IPLOC={}; SUID={}\".format(cookie_params['SNUID'], cookie_params['IPLOC'], cookie_params['SUID']),\n",
    "        \"Host\": \"pb.sogou.com\",\n",
    "        \"Referer\": \"https://weixin.sogou.com/\",\n",
    "        \"User-Agent\": UserAgent\n",
    "    }\n",
    "    response4 = requests.get(url, headers=headers, params=uigs_para)\n",
    "    SetCookie = response4.headers['Set-Cookie']\n",
    "    cookie_params['SUV'] = re.findall('SUV=(.*?);', SetCookie, re.S)[0]\n",
    "    \n",
    "    return cookie_params\n",
    "\n",
    "\n",
    "def get_k_h(url):\n",
    "    b = int(random.random() * 100) + 1\n",
    "    a = url.find(\"url=\")\n",
    "    url = url + \"&k=\" + str(b) + \"&h=\" + url[a + 4 + 21 + b: a + 4 + 21 + b + 1]\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_uigs_para(response):\n",
    "    uigs_para = re.findall('var uigs_para = (.*?);', response.text, re.S)[0]\n",
    "    if 'passportUserId ? \"1\" : \"0\"' in uigs_para:\n",
    "        uigs_para = uigs_para.replace('passportUserId ? \"1\" : \"0\"', '0')\n",
    "    uigs_para = json.loads(uigs_para)\n",
    "    exp_id = re.findall('uigs_para.exp_id = \"(.*?)\";', response.text, re.S)[0]\n",
    "    uigs_para['right'] = 'right0_0'\n",
    "    uigs_para['exp_id'] = exp_id[:-1]\n",
    "    return uigs_para\n",
    "\n",
    "\n",
    "def main_v4(list_url, UserAgent):\n",
    "    headers1 = {\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"zh-CN,zh;q=0.9,en;q=0.8\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Host\": \"weixin.sogou.com\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"User-Agent\": UserAgent,\n",
    "    }\n",
    "    response1 = requests.get(list_url, headers=headers1)\n",
    "    html = etree.HTML(response1.text)\n",
    "    urls = ['https://weixin.sogou.com' + i for i in html.xpath('//div[@class=\"img-box\"]/a/@href')]\n",
    "    \n",
    "    uigs_para = get_uigs_para(response1)\n",
    "    params = get_cookie(response1, uigs_para, UserAgent)\n",
    "    approve_url = 'https://weixin.sogou.com/approve?uuid={}'.format(uigs_para['uuid'])\n",
    "    headers2 = {\n",
    "        \"Accept\": \"*/*\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Accept-Language\": \"zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Cookie\": \"ABTEST={}; IPLOC={}; SUID={}; SUV={}; SNUID={}; JSESSIONID={};\".format(params['ABTEST'], params['IPLOC'],\n",
    "                                                                                          params['SUID'], params['SUV'], params['SNUID'],\n",
    "                                                                                          params['JSESSIONID']),\n",
    "        \"Host\": \"weixin.sogou.com\",\n",
    "        \"Referer\": response1.url,\n",
    "        \"User-Agent\": UserAgent,\n",
    "        \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "    }\n",
    "    for url in urls:\n",
    "        response2 = requests.get(approve_url, headers=headers2)\n",
    "        url = get_k_h(url)\n",
    "        headers3 = {\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "            \"Accept-Language\": \"zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Cookie\": \"ABTEST={}; SNUID={}; IPLOC={}; SUID={}; JSESSIONID={}; SUV={}\".format(params['ABTEST'], params['SNUID'],\n",
    "                                                                                             params['IPLOC'], params['SUID'],\n",
    "                                                                                             params['JSESSIONID'],\n",
    "                                                                                             params['SUV']),\n",
    "            \"Host\": \"weixin.sogou.com\",\n",
    "            \"Referer\": list_url,\n",
    "            \"Upgrade-Insecure-Requests\": \"1\",\n",
    "            \"User-Agent\": UserAgent\n",
    "        }\n",
    "        response3 = requests.get(url, headers=headers3)\n",
    "        \n",
    "        fragments = re.findall(\"url \\+= '(.*?)'\", response3.text, re.S)\n",
    "        itemurl = ''\n",
    "        for i in fragments:\n",
    "            itemurl += i\n",
    "        \n",
    "        # 文章url拿正文\n",
    "        headers4 = {\n",
    "            \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3\",\n",
    "            \"accept-encoding\": \"gzip, deflate, br\",\n",
    "            \"accept-language\": \"zh-CN,zh;q=0.9,en;q=0.8\",\n",
    "            \"cache-control\": \"max-age=0\",\n",
    "            \"user-agent\": UserAgent\n",
    "        }\n",
    "        response4 = requests.get(itemurl, headers=headers4)\n",
    "        html = etree.HTML(response4.text)\n",
    "        print(response4.status_code)\n",
    "        print(html.xpath('//meta[@property=\"og:title\"]/@content')[0])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    key = \"咸蛋超人\"\n",
    "    url = 'https://weixin.sogou.com/weixin?type=2&s_from=input&query={}&_sug_=n&_sug_type_=&page=1'.format(parse.quote(key))\n",
    "    UserAgent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:70.0) Gecko/20100101 Firefox/70.0\"\n",
    "    main_v4(url, UserAgent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节6-4 多线程爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "import urllib.error\n",
    "\n",
    "headers = (\"User-Agent\", \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36\")\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders=[headers]\n",
    "urllib.request.install_opener(opener)\n",
    "for i in range(1,3):\n",
    "    url = \"https://www.qiushibaike.com/8hr/page/\" + str(i)\n",
    "    pagedata = urllib.request.urlopen(url).read().decode(\"utf-8\", \"ignore\")\n",
    "    pat = '<div class=\"content\">.*?<span>(.*?)</span>.*?</div>'\n",
    "    datalist = re.compile(pat, re.S).findall(pagedata)\n",
    "    for j in range(0, len(datalist)):\n",
    "        print(\"第\" + str(i) + \"页第\" + str(j) + \"个段子的内容是：\")\n",
    "        print(datalist[j])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "class A(threading.Thread):\n",
    "    def __init__(self):\n",
    "        threading.Thread.__init__(self)\n",
    "    def run(self):\n",
    "        for i in range(10):\n",
    "            print(\"我是线程A\")\n",
    "class B(threading.Thread):\n",
    "    def __init__(self):\n",
    "        threading.Thread.__init__(self)\n",
    "    def run(self):\n",
    "        for i in range(10):\n",
    "            print(\"我是线程B\")\n",
    "t1 = A()\n",
    "t1.start()\n",
    "t2 = B()\n",
    "t2.start()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 章节6-5 Scrapy框架的安装\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# 章节8-1 天善智能课程自动爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "爬虫难点：1、反爬处理；2、抓包；3、分布式与多线程。\n",
    "验证码处理：1、图像识别；2、外部接口；3、半自动方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "link = 'http://www.santostang.com/'   # 定义link为目标网页地址\n",
    "# 定义请求头的浏览器代理，进行伪装。\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'}\n",
    "# r是requests的Response回复对象\n",
    "r = requests.get(link, headers = headers)\n",
    "# print(r.text)  # r.text是获取的网页内容代码\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")  # 使用BeautifulSoup解析\n",
    "title = soup.find(\"h1\", class_=\"post-title\").a.text.strip()\n",
    "print(title)\n",
    "html = etree.HTML(r.text)  # 使用xpath解析\n",
    "title = html.xpath('//*[@id=\"main\"]/div/div[1]/article[1]/header/h1/a/text()')[0]\n",
    "print(title)\n",
    "# 存储数据到txt\n",
    "with open('test.txt', 'a+') as f:\n",
    "    f.write(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get('http://www.santostang.com/')\n",
    "print('文本编码：' , r.encoding)\n",
    "print('响应状态码：' , r.status_code)\n",
    "print('JSON解码器：' , r.json)\n",
    "print('字节方式的响应体：' , r.content)\n",
    "print('字符串方式的响应体：' , r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "key_dict = {'key1': 'value1', 'key2': 'value2'}\n",
    "r = requests.get('http://httpbin.org/get', params=key_dict)\n",
    "print(\"URL已经正确编码：\", r.url)\n",
    "print(\"字符串方式的响应体：\\n\", r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "headers = {'Host': 'www.santostang.com', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'}\n",
    "r = requests.get('http://www.santostang.com', headers=headers)\n",
    "print('响应状态码：', r.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "key_dict = {'key1': 'value1', 'key2': 'value2'}\n",
    "r = requests.post('http://httpbin.org/post', data=key_dict)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "link = \"http://www.santostang.com/\"\n",
    "r = requests.get(link, timeout = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "\n",
    "def get_movies():\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'}\n",
    "    movie_list = []\n",
    "    for i in range(0, 10):\n",
    "        link = 'https://movie.douban.com/top250?start=' + str(i*25)\n",
    "        r = requests.get(link, headers=headers, timeout=20)\n",
    "        print(str(i+1), '页响应状态码：', r.status_code)\n",
    "        html = etree.HTML(r.text)\n",
    "        title_list = html.xpath('//div[@class=\"hd\"]/a/span[1]/text()')\n",
    "        for each in title_list:\n",
    "            movie_list.append(each)\n",
    "    return movie_list\n",
    "\n",
    "movies = get_movies()\n",
    "print(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def single_page_comment(link):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'}\n",
    "    r = requests.get(link, headers=headers)\n",
    "    json_string = r.text\n",
    "    json_string = json_string[json_string.find('{'):-2]\n",
    "    json_data = json.loads(json_string)\n",
    "    comment_list = json_data['results']['parents']\n",
    "    for each in comment_list:\n",
    "        message = each['content']\n",
    "        print(message)\n",
    "    \n",
    "for page in range(1, 4):\n",
    "    link1 = \"https://api-zero.livere.com/v1/comments/list?callback=jQuery112400642670146062918_1610003898881&limit=10&offset=\"\n",
    "    link2 = \"&repSeq=4272904&requestPath=%2Fv1%2Fcomments%2Flist&consumerSeq=1020&livereSeq=28583&smartloginSeq=5154&code=\"\n",
    "    link = link1 + str(page) + link2\n",
    "    print(\"第\"+str(page)+\"页: \", link)\n",
    "    single_page_comment(link)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "driver = webdriver.Firefox(executable_path = r'D:\\Program Files\\geckodriver\\geckodriver.exe')\n",
    "driver.implicitly_wait(20) # 隐性等待，最长等20秒\n",
    "driver.get(\"http://www.santostang.com/2018/07/04/hello-world/\")\n",
    "# time.sleep(5)\n",
    "# for i in range(0,3):\n",
    "#     driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") # 下滑到页面底部\n",
    "#     # 把所有评论加载出来，首先转换iframe，再找到查看更多，点击\n",
    "#     driver.switch_to.frame(driver.find_element_by_css_selector(\"iframe[title='livere-comment']\"))\n",
    "#     load_more = driver.find_element_by_css_selector('button.more-btn')\n",
    "#     load_more.click()\n",
    "#     # 把iframe又转回去\n",
    "#     driver.switch_to.default_content()\n",
    "#     time.sleep(2)\n",
    "\n",
    "driver.switch_to.frame(driver.find_element_by_css_selector(\"iframe[title='livere-comment']\"))\n",
    "comments = driver.find_elements_by_css_selector('div.reply-content')\n",
    "# print(comment)\n",
    "for eachcomment in comments:\n",
    "    content = eachcomment.find_element_by_tag_name('p')\n",
    "    print(content.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = dirver.find_element_by_name(\"username\")\n",
    "user.clear # 清除元素的内容、\n",
    "user.send_keys(\"123456\") # 模拟按键输入\n",
    "pwd = driver.find_element_by_name(\"password\") # 找到密码输入框\n",
    "pwd.clear\n",
    "pwd.send_keys(\"123456\")\n",
    "dirver.find_element_by_id(\"loginBtn\").click() # 单击登录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//*[@id=\"listing-46887483\"]/div[2]/div[2]/div/div/div[1]/div/span/span/span/span/span[2]\n",
    "https://www.airbnb.cn/s/Shenzhen--China/homes?refinement_paths%5B%5D=%2Fhomes&current_tab_id=home_tab&selected_tab_id=home_tab&screen_size=large&hide_dates_and_guests_filters=false&place_id=ChIJkVLh0Aj0AzQRyYCStw1V7v0&map_toggle=false\n",
    "https://www.airbnb.cn/s/Shenzhen--China/homes?refinement_paths%5B%5D=%2Fhomes&current_tab_id=home_tab&selected_tab_id=home_tab&screen_size=large&hide_dates_and_guests_filters=false&map_toggle=false&place_id=ChIJkVLh0Aj0AzQRyYCStw1V7v0&s_tag=LBUutDen&last_search_session_id=b293d63f-e3fe-4543-8bea-509eff66f7b1&items_offset=20&section_offset=6\n",
    "https://www.airbnb.cn/s/Shenzhen--China/homes?refinement_paths%5B%5D=%2Fhomes&current_tab_id=home_tab&selected_tab_id=home_tab&screen_size=medium&hide_dates_and_guests_filters=false&map_toggle=false&s_tag=LBUutDen&place_id=ChIJkVLh0Aj0AzQRyYCStw1V7v0&last_search_session_id=ddabba5b-96eb-4aa1-b863-255c8759097f&items_offset=40&section_offset=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "driver = webdriver.Firefox(executable_path = r'D:\\Program Files\\geckodriver\\geckodriver.exe')\n",
    "#把上述地址改成你电脑中geckodriver.exe程序的地址\n",
    "#在虚拟浏览器中打开 Airbnb 页面\n",
    "driver.get(\"https://zh.airbnb.com/s/Shenzhen--China/homes\")\n",
    "\n",
    "#找到页面中所有的出租房\n",
    "rent_list = driver.find_elements_by_css_selector('div._gig1e7')\n",
    "\n",
    "#对于每一个出租房\n",
    "for eachhouse in rent_list:\n",
    "    #找到评论数量\n",
    "    try:\n",
    "        comment = eachhouse.find_element_by_css_selector('span._69pvqtq')\n",
    "        comment = comment.text\n",
    "    except:\n",
    "        comment = 0\n",
    "    \n",
    "    #找到价格\n",
    "#     price = eachhouse.find_element_by_css_selector('span._1d8yint7')\n",
    "#     price = price.text.replace(\"每晚\", \"\").replace(\"价格\", \"\").replace(\"\\n\", \"\")\n",
    "    \n",
    "    #找到名称\n",
    "    name = eachhouse.find_element_by_css_selector('div._qrfr9x5')\n",
    "    name = name.text\n",
    "    \n",
    "    #找到房屋类型，大小\n",
    "    details = eachhouse.find_element_by_css_selector('span._faldii7')\n",
    "    details = details.text\n",
    "    house_type = details.split(\" · \")[0]\n",
    "    bed_number = details.split(\" · \")[1]\n",
    "    print (comment, name, house_type, bed_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "driver = webdriver.Firefox(executable_path = r'D:\\Program Files\\geckodriver\\geckodriver.exe')\n",
    "#把上述地址改成你电脑中geckodriver.exe程序的地址\n",
    "for i in range(0,2):\n",
    "    link = \"https://zh.airbnb.com/s/Shenzhen--China/homes?items_offset=\" + str(i * 20)\n",
    "    driver.get(link)\n",
    "    rent_list = driver.find_elements_by_css_selector('div._gig1e7')\n",
    "\n",
    "    for eachhouse in rent_list:\n",
    "        try:\n",
    "            comment = eachhouse.find_element_by_css_selector('span._1clmxfj').text\n",
    "        except:\n",
    "            comment = '缺失'\n",
    "        try:\n",
    "            price = eachhouse.find_element_by_css_selector('div._1ixtnfc')\n",
    "            price = price.text.replace(\"每晚\", \"\").replace(\"价格\", \"\").replace(\"\\n\", \"\")\n",
    "        except:\n",
    "            price = '缺失'\n",
    "        try:\n",
    "            name = eachhouse.find_element_by_css_selector('div._qrfr9x5')\n",
    "            name = name.text\n",
    "        except:\n",
    "            name = '缺失'\n",
    "        try:\n",
    "            details = eachhouse.find_element_by_css_selector('span._faldii7')\n",
    "            details = details.text\n",
    "            house_type = details.split(\" · \")[0]\n",
    "            bed_number = details.split(\" · \")[1]\n",
    "        except:\n",
    "            house_type, bed_number = '缺失', '缺失'\n",
    "        print (comment, price, name, house_type, bed_number)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = '[0-9]+'\n",
    "line = 'The first number is 12345, 12345 is the second.'\n",
    "print(re.match(pattern, line))\n",
    "print(re.search(pattern, line))\n",
    "print(re.findall(pattern, line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.98 Safari/537.36'}\n",
    "for i in range(1,11):\n",
    "    link = 'https://beijing.anjuke.com/sale/p' + str(i)\n",
    "    r = requests.get(link, headers = headers)\n",
    "    print ('现在爬取的是第', i, '页')\n",
    "\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    house_list = soup.find_all('li', class_=\"list-item\")\n",
    "\n",
    "    for house in house_list:\n",
    "        try:\n",
    "            name = house.find('div', class_ ='house-title').a.text.strip()\n",
    "            price = house.find('span', class_='price-det').text.strip()\n",
    "            price_area = house.find('span', class_='unit-price').text.strip()\n",
    "\n",
    "            no_room = house.find('div', class_='details-item').span.text\n",
    "            area = house.find('div', class_='details-item').contents[3].text\n",
    "            floor = house.find('div', class_='details-item').contents[5].text\n",
    "            year = house.find('div', class_='details-item').contents[7].text\n",
    "            broker = house.find('span', class_='broker-name broker-text').text\n",
    "            broker = broker[1:]\n",
    "            address = house.find('span', class_='comm-address').text.strip()\n",
    "            address = address.replace('\\xa0\\xa0\\n                    ','  ')\n",
    "            tag_list = house.find_all('span', class_='item-tags')\n",
    "            tags = [i.text for i in tag_list] \n",
    "            print (name, price, price_area, no_room, area, floor, year, broker, address, tags)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "插入成功！\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "# 打开数据库连接\n",
    "conn = pymysql.connect(host=\"127.0.0.1\", user=\"root\", password=\"root123456.\", db=\"dangdang\")\n",
    "# SQL操作\n",
    "sql = \"INSERT INTO books(title, link, cost, bookshop, comment) VALUES(%s, %s, %s, %s, %s)\"\n",
    "param = (\"test2\", \"test2\", \"test\", \"test\", \"test\")\n",
    "try:\n",
    "    # 使用cursor()方法获取操作游标\n",
    "    cursor = conn.cursor()\n",
    "    # 执行sql语句\n",
    "    cursor.execute(sql, param)\n",
    "    # 提交到数据库执行\n",
    "    conn.commit()\n",
    "    print(\"插入成功！\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    # 如果发生错误则回滚\n",
    "    conn.rollback()\n",
    "finally:\n",
    "    # 关闭数据库连接\n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.blog_database\n",
    "collection = db.blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.blog_database\n",
    "collection = db.blog\n",
    "link = \"http://www.santostang.com/\"\n",
    "headers = {'User-Agent' : 'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'} \n",
    "r = requests.get(link, headers= headers)\n",
    "soup = BeautifulSoup(r.text, \"lxml\")\n",
    "title_list = soup.find_all(\"h1\", class_=\"post-title\")\n",
    "for eachone in title_list:\n",
    "    url = eachone.a['href']\n",
    "    title = eachone.a.text.strip()\n",
    "    post = {\"url\": url, \"title\": title, \"data\": datetime.datetime.utcnow()}\n",
    "    collection.insert_one(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[置顶]\\n【虎扑-天选之子】冬日送温暖，iphone12mini免费送\\n\\xa0\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...15\\n\\n\\xa0]', 'https://bbs.hupu.com/40346769.html', '天选之子官方', 'https://my.hupu.com/242532589161998', '2021-01-08', '288', '2186553', '满眼星辰皆是陳', datetime.datetime(2021, 1, 12, 16, 39)]\n",
      "['学校这位卧龙什么水平？\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...8\\n\\n\\xa0]', 'https://bbs.hupu.com/40424541.html', '说不过就别接着杠啦', 'https://my.hupu.com/130313337891224', '2021-01-12', '143', '155538', '虎扑JR1911883317', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['《传奇》这游戏已经有20多年了吧，咋还有人玩呢，好在哪。。。\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...9\\n\\n\\xa0]', 'https://bbs.hupu.com/40402904.html', '一炮干哈炕', 'https://my.hupu.com/208290034176260', '2021-01-11', '169', '102834', '山羊不如绵羊', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['女朋友被陌生人在微博侮辱，只能请家人们出主意了！ \\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...7\\n\\n\\xa0]', 'https://bbs.hupu.com/40422577.html', '麦子灬马刺', 'https://my.hupu.com/87691030988142', '2021-01-12', '129', '43049', '20200911秃了也变憨了', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['什么时候才能真正的长大？该怎么教育？\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...30\\n\\n\\xa0]', 'https://bbs.hupu.com/40421031.html', '欢乐贩卖基地', 'https://my.hupu.com/69543557767402', '2021-01-12', '589', '581284', 'Angst丶', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['一句话证明你认识这个演员\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...9\\n\\n\\xa0]', 'https://bbs.hupu.com/40420135.html', '油漆区风云nismo', 'https://my.hupu.com/164598804267645', '2021-01-12', '163', '328790', 'Maskcy', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['我也是有腹肌的人了哈哈\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...11\\n\\n\\xa0]', 'https://bbs.hupu.com/40401149.html', 'Frankwnx0511', 'https://my.hupu.com/277092181602609', '2021-01-11', '204', '260152', '一枚熊密', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['各位兄台，茅台到底咋抢啊？\\n[\\xa0\\n2\\n\\n\\xa0]', 'https://bbs.hupu.com/40423733.html', 'BitTorrentZ', 'https://my.hupu.com/99411734407153', '2021-01-12', '21', '9398', 'guseyidi', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['JR们，女生这样问你，你们会怎么回答？\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n\\n\\xa0]', 'https://bbs.hupu.com/40417558.html', '网瘾杀手杨永信', 'https://my.hupu.com/225302442687880', '2021-01-11', '34', '27393', '虎扑JR1543629165', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['第一次发帖 实在想不明白 男朋友为什么会在虎扑评论区撩别的女生 真的是虎扑的风气吗', 'https://bbs.hupu.com/40427744.html', 'GutieLu', 'https://my.hupu.com/32080974772577', '2021-01-12', '3', '114', 'GutieLu', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['这个感觉还行\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3\\n4\\n5\\n\\n\\xa0]', 'https://bbs.hupu.com/40425638.html', '德云基地', 'https://my.hupu.com/53519744250979', '2021-01-12', '90', '81942', '二不二不二不二', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['好家伙，虎扑ID终于还是被身边的朋友们发现了\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n\\n\\xa0]', 'https://bbs.hupu.com/40424438.html', 'Summer是我', 'https://my.hupu.com/88408987648212', '2021-01-12', '34', '38881', 'AntiMaga', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['河北的高考竞争我还以为能排全国前三\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n\\n\\xa0]', 'https://bbs.hupu.com/40423637.html', '6663275', 'https://my.hupu.com/178956008479759', '2021-01-12', '32', '10335', 'MaxLowe', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['二战中的法国，到底是不是两边下注？谁赢了都不亏。\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n\\n\\xa0]', 'https://bbs.hupu.com/40421963.html', '用户2052593806', 'https://my.hupu.com/248672222685768', '2021-01-12', '37', '34936', '虎扑JR0088031862', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['同时有前列腺炎＋痔疮＋牙龈炎＋肾结石是不是可以重开了？\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...6\\n\\n\\xa0]', 'https://bbs.hupu.com/40389394.html', '普罗泰格拉', 'https://my.hupu.com/224324738862403', '2021-01-10', '104', '173385', '灵魂是自由的', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['月薪6500，感觉自己像废人...', 'https://bbs.hupu.com/40427344.html', '苏州黄先生', 'https://my.hupu.com/101660934442073', '2021-01-12', '12', '5169', '火箭名宿富儿子', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['xdm，槟榔在你们那火不火', 'https://bbs.hupu.com/40424864.html', '米线加个蛋', 'https://my.hupu.com/35322575412690', '2021-01-12', '7', '2915', '哎呦朕屮', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['我出轨了，求骂醒…\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...19\\n\\n\\xa0]', 'https://bbs.hupu.com/40422467.html', '虎扑JR0288827773', 'https://my.hupu.com/217695394668918', '2021-01-12', '366', '217830', '300块回HK', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['现金一两百万的家庭是什么家庭\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...13\\n\\n\\xa0]', 'https://bbs.hupu.com/40308177.html', '捡破烂的王', 'https://my.hupu.com/153251844987925', '2021-01-06', '253', '187031', 'Smaster0708', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['出社会才知道老师教的东西多好用', 'https://bbs.hupu.com/40426746.html', '苗可秀', 'https://my.hupu.com/135435097891756', '2021-01-12', '19', '16616', 'kindofaki', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['如果你是机长，用一句话让乘客不安\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...7\\n\\n\\xa0]', 'https://bbs.hupu.com/40421179.html', '太浪别太杠', 'https://my.hupu.com/59829668260059', '2021-01-12', '132', '150329', '宇宙第一窃魂卷', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['各位老哥，杰士邦又出了一款绝对会买爆的新产品\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...83\\n\\n\\xa0]', 'https://bbs.hupu.com/40420671.html', '杰士邦官方账号', 'https://my.hupu.com/34192304359566', '2021-01-12', '1652', '496247', '舒克和路飞', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['怎么能让“老婆”开心，这招百试百灵啊\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n\\n\\xa0]', 'https://bbs.hupu.com/40416898.html', '湖人总冠军我准了', 'https://my.hupu.com/117437118394549', '2021-01-11', '26', '34373', '虎扑JR0742291178', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['当年《家有儿女》他们的家庭水平  放到现在是什么水平\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...15\\n\\n\\xa0]', 'https://bbs.hupu.com/40424380.html', '霍景耀', 'https://my.hupu.com/89248052322998', '2021-01-12', '280', '170010', '挨最毒的打', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['大家快买东莞房！\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3\\n\\n\\xa0]', 'https://bbs.hupu.com/40422882.html', '勒布朗只有一个女人', 'https://my.hupu.com/146635543716607', '2021-01-12', '45', '26827', '易水寒跳起一刀', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['谁说俄罗斯女人不怕冷?中国的冬天就给治卑服的。。。\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n\\n\\xa0]', 'https://bbs.hupu.com/40414793.html', '宝兴街霸', 'https://my.hupu.com/182750025367434', '2021-01-11', '36', '21238', '卢单语', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['法硕非法学硕士何去何从\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3\\n\\n\\xa0]', 'https://bbs.hupu.com/40400136.html', 'loofahstu', 'https://my.hupu.com/137505286081035', '2021-01-11', '46', '32104', '虎扑JR0881819529', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['这孩子唱的这首《踏山河》怎么样\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n\\n\\xa0]', 'https://bbs.hupu.com/40417901.html', '虎扑JR0032397169', 'https://my.hupu.com/124138830680489', '2021-01-11', '25', '25212', '看球聊人生', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['本科生是不是很难赚到街薪\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3\\n\\n\\xa0]', 'https://bbs.hupu.com/40424710.html', '小乌龟圆滚滚', 'https://my.hupu.com/114095650042134', '2021-01-12', '55', '18443', '大工菜虚坤', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['给蜀国方便面的脑洞还真有人写成书了', 'https://bbs.hupu.com/40425091.html', '民以食为关', 'https://my.hupu.com/230764000857069', '2021-01-12', '10', '4401', '虎扑JR1317689881', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['刚过一天就打脸了\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...14\\n\\n\\xa0]', 'https://bbs.hupu.com/40412462.html', '救救板栗', 'https://my.hupu.com/88768377115135', '2021-01-11', '262', '333358', '尼克杨才是老大', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['中国历朝代版图大全\\n\\n\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3\\n4\\n5\\n\\n\\xa0]', 'https://bbs.hupu.com/40385777.html', '冰帝迹部', 'https://my.hupu.com/35437063005932', '2021-01-10', '93', '262899', '哭就狗丶坐公狮', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['很久没留意股市了，现在才知道.......', 'https://bbs.hupu.com/40426439.html', '西藏的天', 'https://my.hupu.com/211825824954502', '2021-01-12', '11', '6276', 'Voicer', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['给jrs报喜啦，小弟过了pmp\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3\\n4\\n5\\n\\n\\xa0]', 'https://bbs.hupu.com/40420483.html', '大鹏子啊大鹏子', 'https://my.hupu.com/13937023747556', '2021-01-12', '92', '184680', 'Useven', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['现实中这样的气质和颜值彩礼得要多少？\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...8\\n\\n\\xa0]', 'https://bbs.hupu.com/40419427.html', '单依纯老公', 'https://my.hupu.com/213860870866583', '2021-01-12', '146', '255237', '詹姆斯007邦德', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['这个孩子的名字自带特殊光环，连老师都不敢点名，你认为叫什麽?\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...21\\n\\n\\xa0]', 'https://bbs.hupu.com/40346402.html', '虎扑JR1656047714', 'https://my.hupu.com/72674489332702', '2021-01-08', '403', '350251', '一身正气两袖清风', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['某车站内两人不配合检查，与车站工作人员发生冲突\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3\\n4\\n5\\n\\n\\xa0]', 'https://bbs.hupu.com/40425306.html', '克塞基地', 'https://my.hupu.com/268481056205317', '2021-01-12', '91', '48861', '只莫不甘娜不行哦', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['现在女生都必须打宫颈癌疫苗吗？\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3\\n\\n\\xa0]', 'https://bbs.hupu.com/40424552.html', '世间有我真幸运', 'https://my.hupu.com/221179013951525', '2021-01-12', '59', '22533', '世间有我真幸运', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['苏州微软\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3\\n\\n\\xa0]', 'https://bbs.hupu.com/40417033.html', 'Jerry0513', 'https://my.hupu.com/149882500103165', '2021-01-11', '40', '38243', 'KobeRIPBryant', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['希望疫情不要卷土重来！\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...6\\n\\n\\xa0]', 'https://bbs.hupu.com/40398179.html', 'KylieIrving', 'https://my.hupu.com/189411883466272', '2021-01-10', '114', '97275', '韩路', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['现在网上说的名媛到底是啥意思？指家里有钱吗？', 'https://bbs.hupu.com/40426560.html', '海天一键', 'https://my.hupu.com/44694160892733', '2021-01-12', '13', '10553', '和尚洗头就用飘柔', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['一口水看了八集上阳赋，忍不住要说\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3\\n\\n\\xa0]', 'https://bbs.hupu.com/40383163.html', '可为你逆天', 'https://my.hupu.com/39255061936177', '2021-01-10', '53', '60040', '可为你逆天', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['记录一下第一个自然年工资，三线城市小国企', 'https://bbs.hupu.com/40427055.html', '董dbl', 'https://my.hupu.com/62540412356705', '2021-01-12', '15', '3005', '董dbl', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['谢孟伟搞笑续集\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3\\n4\\n\\n\\xa0]', 'https://bbs.hupu.com/40423331.html', 'ZAIV8', 'https://my.hupu.com/279517910385288', '2021-01-12', '62', '110322', '奉公守法好公民', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['在黑龙江买一套别墅仅需26万是一种怎么样的体验\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...8\\n\\n\\xa0]', 'https://bbs.hupu.com/40422561.html', '中国的鹏鹏哥', 'https://my.hupu.com/232233002043132', '2021-01-12', '152', '119870', '蓝梦岛的夜晚', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['如果发明一个能存储睡眠的东西，会不会成为中国首富？\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...8\\n\\n\\xa0]', 'https://bbs.hupu.com/40411696.html', '灵机一动一动动', 'https://my.hupu.com/214569626974086', '2021-01-11', '150', '320481', '一念轻安', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['红烧肉，有没有哪位想尝尝', 'https://bbs.hupu.com/40427870.html', '老高评事', 'https://my.hupu.com/24471297874185', '2021-01-12', '0', '4389', '老高评事', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['终于轮到我了，在和福禄娃合伙创业，JRS有什么要问？', 'https://bbs.hupu.com/40427127.html', '羊总', 'https://my.hupu.com/237676390427645', '2021-01-12', '15', '2457', '哈萨克斯坦歌王迪玛希', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['铁木真明明更喜欢并且更信任拖雷，而且让他监国，为何临终前传位窝阔台而不是托雷呢？', 'https://bbs.hupu.com/40426450.html', '湖人卫冕总冠军鹏', 'https://my.hupu.com/193045388117404', '2021-01-12', '8', '9148', '米老呆', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['湖北市民送大肥猪给消防员表达救命之恩\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3\\n4\\n\\n\\xa0]', 'https://bbs.hupu.com/40424684.html', '海边居民', 'https://my.hupu.com/25272437672007', '2021-01-12', '65', '97059', 'qpcjay', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['电动车续航1000公里？油车是不是要被取代了\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...14\\n\\n\\xa0]', 'https://bbs.hupu.com/40392265.html', '我绝不会倒下', 'https://my.hupu.com/53373931609832', '2021-01-10', '268', '275247', '猫头姐', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['孩子说南方话真的会影响嘴型吗？', 'https://bbs.hupu.com/40425148.html', '人文历史和艺术', 'https://my.hupu.com/227260674922667', '2021-01-12', '13', '10596', '我爱湖人kb', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['终究是没躲过\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3\\n4\\n5\\n\\n\\xa0]', 'https://bbs.hupu.com/40424812.html', 'Sheila_Liu刘星兰', 'https://my.hupu.com/184047362782429', '2021-01-12', '98', '63170', '狄奥多西城墙', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['假如给你这么一份年终奖，你会不会休？\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...11\\n\\n\\xa0]', 'https://bbs.hupu.com/40402821.html', '欢乐贩卖基地', 'https://my.hupu.com/69543557767402', '2021-01-11', '219', '681350', '北海北丨', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['同时报考广东选调，国考，深圳市考居然会导致这样的问题!\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n\\n\\xa0]', 'https://bbs.hupu.com/40424250.html', 'Valu6', 'https://my.hupu.com/19860034273314', '2021-01-12', '27', '9180', 'Valu6', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['让鲁迅去考高中语文，能考多少分', 'https://bbs.hupu.com/40403537.html', '我说不要就是要', 'https://my.hupu.com/253691215574975', '2021-01-11', '11', '3977', '16134909', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['有没有学医的老哥 帮帮忙', 'https://bbs.hupu.com/40427200.html', '我先去洗澡吧', 'https://my.hupu.com/22948778535763', '2021-01-12', '7', '3368', '虎扑JR0287319380', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['毕业五年，电厂倒班狗，坐标宁夏，这收入及格吗\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3\\n\\n\\xa0]', 'https://bbs.hupu.com/40426236.html', '虎扑JR1674448118', 'https://my.hupu.com/190783188678701', '2021-01-12', '44', '15826', '虎扑JR1674448118', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['这比我们蓝翔的新年联欢会强多了', 'https://bbs.hupu.com/40425519.html', '潇潇逸水寒', 'https://my.hupu.com/13773131828791', '2021-01-12', '19', '12276', '清风之刃', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['今年的集五福活动又要来了\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3\\n\\n\\xa0]', 'https://bbs.hupu.com/40425250.html', '李三飞', 'https://my.hupu.com/111410082082418', '2021-01-12', '42', '18581', '买fake的都是吊丝', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['昨晚学着街上的jr网易云匹配 结果昨晚的留言现在才发现\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3\\n\\n\\xa0]', 'https://bbs.hupu.com/40418420.html', '最强的怪兽', 'https://my.hupu.com/87224867910600', '2021-01-12', '57', '87075', '冷饮口感更佳', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['有哪些行业是不可能被互联网收割的？', 'https://bbs.hupu.com/40427683.html', '柚小哥', 'https://my.hupu.com/238913180309718', '2021-01-12', '3', '2743', 'Anti996', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['这个叉烧双拼饭怎么样？\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...18\\n\\n\\xa0]', 'https://bbs.hupu.com/40422880.html', '苗可秀', 'https://my.hupu.com/135435097891756', '2021-01-12', '357', '260583', '带吾去旅行', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['落袋为安，再见btc\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...9\\n\\n\\xa0]', 'https://bbs.hupu.com/40421315.html', 'LeoMaNT', 'https://my.hupu.com/76446709948563', '2021-01-12', '179', '267449', '虎扑JR0179736440', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['肺部健康小测试：憋气测试\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n\\n\\xa0]', 'https://bbs.hupu.com/40421383.html', '脚麻的蜗牛', 'https://my.hupu.com/9591216752268', '2021-01-12', '36', '24308', '无聊的阿恩', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['名场面：郭杰瑞测评顺德牛展，惨遭赶出~\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3\\n4\\n5\\n\\n\\xa0]', 'https://bbs.hupu.com/40425372.html', '大哥说我去会会他们', 'https://my.hupu.com/69688486215225', '2021-01-12', '98', '108225', '医院骑士', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['JRS们，分手了，有个搞不明白的问题', 'https://bbs.hupu.com/40423463.html', '贵族灬成哥', 'https://my.hupu.com/241949392452122', '2021-01-12', '9', '6064', '给我一支烟的时间', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['江湖戏法——水漫金山', 'https://bbs.hupu.com/40427858.html', '清纯基地', 'https://my.hupu.com/206104842192446', '2021-01-12', '0', '4389', '清纯基地', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['有没有看出来怎么弄的？', 'https://bbs.hupu.com/40427793.html', '清纯基地', 'https://my.hupu.com/206104842192446', '2021-01-12', '0', '30', '清纯基地', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['经典MV，从不让你失望', 'https://bbs.hupu.com/40427620.html', '美女基地', 'https://my.hupu.com/205226121481456', '2021-01-12', '0', '817', '美女基地', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['Nancy被偷拍？', 'https://bbs.hupu.com/40427609.html', '美女基地', 'https://my.hupu.com/205226121481456', '2021-01-12', '1', '865', '美女基地', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['委屈中带着倔强！', 'https://bbs.hupu.com/40427536.html', '美女基地', 'https://my.hupu.com/205226121481456', '2021-01-12', '0', '1321', '美女基地', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['给阿姨倒一杯卡布奇诺', 'https://bbs.hupu.com/40427393.html', '枫潇潇汐', 'https://my.hupu.com/272547241851590', '2021-01-12', '3', '2258', '虎扑JR1482343448', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['理性讨论 985/211均分78、79和双非一本甚至二本均分85+哪个竞争力更大（相同专业）', 'https://bbs.hupu.com/40426575.html', 'Crazy1997', 'https://my.hupu.com/26685193422286', '2021-01-12', '4', '2213', 'cuecueme', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['邻居老在楼道抽烟，怎么办？\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...19\\n\\n\\xa0]', 'https://bbs.hupu.com/40391464.html', 'degayes号被盗', 'https://my.hupu.com/119549661004740', '2021-01-10', '379', '229930', '科比干拔后仰跳楼', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['狗子:这只猫的个头有点大啊！\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...13\\n\\n\\xa0]', 'https://bbs.hupu.com/40039709.html', '拾荒之神毁人不倦', 'https://my.hupu.com/269300857140009', '2020-12-28', '250', '468217', '专心聊会天', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['深度还原中年男人办事的方式\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3\\n\\n\\xa0]', 'https://bbs.hupu.com/40423371.html', '纳什Steve', 'https://my.hupu.com/206521611773256', '2021-01-12', '41', '83602', '纳什均衡教派', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['2020年的收入  坐标惠州 感谢努力的自己\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...7\\n\\n\\xa0]', 'https://bbs.hupu.com/40420917.html', '南国有鱼其名为琨', 'https://my.hupu.com/80307338290596', '2021-01-12', '128', '236412', 'RaptorsLowry7', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['你们身边有没有乱杀的实例？\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3\\n4\\n\\n\\xa0]', 'https://bbs.hupu.com/40417883.html', '用户0760887331', 'https://my.hupu.com/139957895007005', '2021-01-11', '65', '57848', '赌徒特酿', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['上学不努力，长大电子厂，10年工作经验电子厂真实工资\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...18\\n\\n\\xa0]', 'https://bbs.hupu.com/40383643.html', '祝福我2017', 'https://my.hupu.com/183528353993936', '2021-01-10', '346', '319348', '江山如歌', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['女生卸完妆到底有多丑？', 'https://bbs.hupu.com/40426772.html', '世间有我真幸运', 'https://my.hupu.com/221179013951525', '2021-01-12', '6', '2116', '季昌明', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['量力而行啊喵喵，你打得过它吗？', 'https://bbs.hupu.com/40423021.html', '美女基地的小秘书', 'https://my.hupu.com/122284757116348', '2021-01-12', '6', '20892', '25岁前一定要结束牡丹', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['xdm车厘子是进口来的安全吗', 'https://bbs.hupu.com/40417225.html', '重力魔王卡文迪许', 'https://my.hupu.com/270314995111786', '2021-01-11', '13', '18351', '闭眼装天黑', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['兄弟们这是啥品种的狗啊', 'https://bbs.hupu.com/40426040.html', '粉粉的笔', 'https://my.hupu.com/51754020277855', '2021-01-12', '16', '3002', '蛆吧对线之王', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['是祸躲不过，视频车要负责吗\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3\\n4\\n5\\n\\n\\xa0]', 'https://bbs.hupu.com/40423506.html', '崛起吧中国男篮', 'https://my.hupu.com/38131780240011', '2021-01-12', '80', '177549', '不愿透露姓名的唐马儒先生', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['大专毕业3年，我还要去提升学历吗？\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3\\n4\\n5\\n\\n\\xa0]', 'https://bbs.hupu.com/40397724.html', 'triir', 'https://my.hupu.com/148332497748922', '2021-01-10', '99', '73123', '彩彩子哟', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['缅甸的女人顶呱呱', 'https://bbs.hupu.com/40362291.html', 'dfblr1', 'https://my.hupu.com/11336887956641', '2021-01-09', '17', '24180', '肥wall', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['卧槽。。。2021第一个大瓜，TED和INFI怼起来了？', 'https://bbs.hupu.com/40423977.html', 'zeus177169', 'https://my.hupu.com/157912555521477', '2021-01-12', '7', '792', 'zeus177169', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['第二轮投票是不是有工作人员介入啊', 'https://bbs.hupu.com/40426532.html', '拉车专业户', 'https://my.hupu.com/113257751481460', '2021-01-12', '5', '2412', '塑料战舰', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['说到“相声”，你会想到什么', 'https://bbs.hupu.com/40425585.html', '20一RayAllen一34', 'https://my.hupu.com/246411793401588', '2021-01-12', '18', '7503', '空门打飞', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['不提美，漂亮，怎么形容一个女生很美\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...6\\n\\n\\xa0]', 'https://bbs.hupu.com/40425524.html', '你可以永远相信懒羊羊', 'https://my.hupu.com/20957287862513', '2021-01-12', '103', '46927', '抽烟烂肺', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['兄弟把我绿了？\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3\\n4\\n\\n\\xa0]', 'https://bbs.hupu.com/40422819.html', '虎扑JR1857967760', 'https://my.hupu.com/172520320823650', '2021-01-12', '64', '62363', '阁楼一梦', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['这是在跳舞还是健身？', 'https://bbs.hupu.com/40427863.html', '海天一键', 'https://my.hupu.com/44694160892733', '2021-01-12', '0', '2', '海天一键', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['如何评价徐阶、高拱和张居正这三个明朝首辅的关系?\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...7\\n\\n\\xa0]', 'https://bbs.hupu.com/40405554.html', '天降猛男马保国', 'https://my.hupu.com/238056442907401', '2021-01-11', '139', '143327', '天堂向左spawN', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['终于轮到我了！我是汉语6级的巴柔冠军，今年即将成为中国女婿，进来聊聊吗？\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...27\\n\\n\\xa0]', 'https://bbs.hupu.com/40422893.html', 'BJJ威尔教练', 'https://my.hupu.com/173024888790652', '2021-01-12', '532', '401987', 'miller45', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['还有吃青苔的？\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...6\\n\\n\\xa0]', 'https://bbs.hupu.com/40420409.html', '我的老婆是林熙蕾', 'https://my.hupu.com/139803316879215', '2021-01-12', '103', '136932', '星语0321', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['工行房贷提前还款需支付违约金', 'https://bbs.hupu.com/40401189.html', 'GXLBJ', 'https://my.hupu.com/233499355054992', '2021-01-11', '9', '4473', '蒂尔曼非尔蒂塔', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['疫情下的石家庄\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...34\\n\\n\\xa0]', 'https://bbs.hupu.com/40355003.html', 'paullang', 'https://my.hupu.com/190783185774228', '2021-01-08', '676', '570714', '柯南是只小肥猪', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['杭州姑娘  诚招对象', 'https://bbs.hupu.com/40426645.html', '胡子拉二胡', 'https://my.hupu.com/83938800280302', '2021-01-12', '19', '3808', '一只大狸猫', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['隔离酒店的群里要笑死我了\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n\\n\\xa0]', 'https://bbs.hupu.com/40426162.html', 'ShanDong彭于晏', 'https://my.hupu.com/66665765311151', '2021-01-12', '30', '58640', '丹麦队长', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['因为不喜欢女朋友爸妈分手了\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...40\\n\\n\\xa0]', 'https://bbs.hupu.com/40416472.html', '小嘟嘟的大川', 'https://my.hupu.com/64423737547117', '2021-01-11', '799', '872698', 'susu180', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['国企年薪税后17w；医院后勤事业编，该怎么选？\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n\\n\\xa0]', 'https://bbs.hupu.com/40415129.html', '虎扑JR1066338027', 'https://my.hupu.com/259011798878661', '2021-01-11', '23', '12667', '傻呆秃的胖面瘫', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['“耐造”是不是谐音“耐操”', 'https://bbs.hupu.com/40427862.html', '虎扑JR0772313444', 'https://my.hupu.com/7513430574378', '2021-01-12', '0', '3', '虎扑JR0772313444', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['连续失眠五天 人都傻了', 'https://bbs.hupu.com/40419718.html', '老友记永远滴神', 'https://my.hupu.com/39978842269132', '2021-01-12', '6', '2120', '我是真船迷123', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['都是兄弟，一起来薅社会主义羊毛（网上国网）', 'https://bbs.hupu.com/40425075.html', 'ilove12', 'https://my.hupu.com/8846818641360', '2021-01-12', '8', '1411', 'ilove12', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['明天双方父母就要见面了，今天却因为彩礼的事情睡不着\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...15\\n\\n\\xa0]', 'https://bbs.hupu.com/40419074.html', 'Zz我是笑笑', 'https://my.hupu.com/117115038944050', '2021-01-12', '295', '305918', '黄牌拉莫斯', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['红衣男子：我到站了。  人群：没有 你还没到站\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...15\\n\\n\\xa0]', 'https://bbs.hupu.com/40362460.html', '我的老婆是林熙蕾', 'https://my.hupu.com/139803316879215', '2021-01-09', '292', '525547', '鼻涕泡饭', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['有在济南的家人吗，我有两个乌龟带不回去了，可以来帮我存一下或者寄个快递吗？\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...17\\n\\n\\xa0]', 'https://bbs.hupu.com/40422912.html', '虎扑JR0009595236', 'https://my.hupu.com/220287672495814', '2021-01-12', '329', '222247', 'Illegal_High', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['调查统计贴，你春节还回家过年吗？\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...46\\n\\n\\xa0]', 'https://bbs.hupu.com/40402462.html', '平平无奇kop', 'https://my.hupu.com/87580301114656', '2021-01-11', '904', '784440', '合肥胡歌', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['乡下人进城了', 'https://bbs.hupu.com/40427861.html', '国内知名赶高铁运动员', 'https://my.hupu.com/74986593900081', '2021-01-12', '0', '4390', '国内知名赶高铁运动员', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['前方笑点高能，请做好准备！ \\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...14\\n\\n\\xa0]', 'https://bbs.hupu.com/40296005.html', '枫潇潇汐', 'https://my.hupu.com/272547241851590', '2021-01-05', '274', '697586', 'GCrow', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['一句话证明你认识他\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...6\\n\\n\\xa0]', 'https://bbs.hupu.com/40424601.html', '隔壁晚上那么吵', 'https://my.hupu.com/213584952954799', '2021-01-12', '108', '35275', '请升级权限', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['关于事业编\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n\\n\\xa0]', 'https://bbs.hupu.com/40417283.html', '抱着艺艺的腿睡觉', 'https://my.hupu.com/45062872702', '2021-01-11', '26', '27427', '一个文', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['兄弟们求助，刚刷了miui的新系统\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3\\n\\n\\xa0]', 'https://bbs.hupu.com/40410404.html', '豚鼠', 'https://my.hupu.com/129542825670907', '2021-01-11', '43', '22326', '木狼嘴哥mvp', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['国考面试求教', 'https://bbs.hupu.com/40422879.html', '可爱的小宝宝宝', 'https://my.hupu.com/183991882074137', '2021-01-12', '20', '21219', '大爱麦迪MVP', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['女孩子来例假最希望男孩子说什么？\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3...36\\n\\n\\xa0]', 'https://bbs.hupu.com/40421423.html', '暴走苍穹z', 'https://my.hupu.com/168488805363875', '2021-01-12', '714', '442312', '曹达华软饭硬吃', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['女生看的一米八几的男生就自卑了\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3\\n\\n\\xa0]', 'https://bbs.hupu.com/40418547.html', '雪山飞狐西门吹雪', 'https://my.hupu.com/5200894839419', '2021-01-12', '43', '97927', '俾钱我帮你做嘢', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['韩式煎蛋炒饭，做法和中式炒饭有何不同？\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n3\\n4\\n5\\n\\n\\xa0]', 'https://bbs.hupu.com/40384837.html', '帕了个露露', 'https://my.hupu.com/140906929863167', '2021-01-10', '82', '35867', '0770ljy', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['Jr们一般多少岁经济独立？', 'https://bbs.hupu.com/40427196.html', '开心果甜甜圈', 'https://my.hupu.com/173966353693726', '2021-01-12', '16', '6197', '拉风的PIE', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['陈佩斯车站拉客，被穆铁柱活捉', 'https://bbs.hupu.com/40425292.html', '苗可秀', 'https://my.hupu.com/135435097891756', '2021-01-12', '10', '8399', '永遠不變的風景', datetime.datetime(2021, 1, 12, 16, 45)]\n",
      "['毛晓彤，真是好女孩啊\\n\\n\\xa0\\n\\n[\\xa0\\n2\\n\\n\\xa0]', 'https://bbs.hupu.com/40423646.html', '落幕Ending', 'https://my.hupu.com/85136773049732', '2021-01-12', '25', '24357', '用华为我荣耀', datetime.datetime(2021, 1, 12, 16, 45)]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# 获取页面\n",
    "def get_page(link):\n",
    "    headers = {'User-Agent' : 'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'} \n",
    "    r = requests.get(link, headers = headers)\n",
    "    html = r.content  #使用r.content解封装\n",
    "    html = html.decode('utf-8')  #由UTF-8解码为unicode\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    return soup\n",
    "\n",
    "# 解析网页\n",
    "def get_data(post_list):\n",
    "    data_list =[]\n",
    "    for post in post_list:\n",
    "        title = post.find('div',class_='titlelink box').text.strip()\n",
    "        post_link = post.find('div',class_='titlelink box').a['href']\n",
    "        post_link = \"https://bbs.hupu.com\" + post_link \n",
    "\n",
    "        author = post.find('div',class_='author box').a.text.strip()\n",
    "        author_page = post.find('div',class_='author box').a['href']\n",
    "        start_date = post.find('div',class_='author box').contents[5].text.strip()\n",
    "\n",
    "        reply_view = post.find('span',class_='ansour box').text.strip()\n",
    "        reply = reply_view.split('/')[0].strip()\n",
    "        view = reply_view.split('/')[1].strip()\n",
    "\n",
    "        reply_time = post.find('div',class_='endreply box').a.text.strip()\n",
    "        last_reply = post.find('div',class_='endreply box').span.text.strip()\n",
    "        if ':' in reply_time: #时间是11:27\n",
    "            date_time = str(datetime.date.today()) + ' ' + reply_time\n",
    "            date_time = datetime.datetime.strptime(date_time, '%Y-%m-%d %H:%M')\n",
    "        elif reply_time.find(\"-\") == 4: #时间是2017-02-27\n",
    "            date_time = datetime.datetime.strptime(reply_time, '%Y-%m-%d').date()\n",
    "        else: #时间是11-27\n",
    "            date_time = datetime.datetime.strptime('2018-' + reply_time, '%Y-%m-%d').date()\n",
    "        data_list.append([title, post_link, author, author_page, start_date, reply, view, last_reply, date_time])\n",
    "    return data_list\n",
    "\n",
    "class MongoAPI(object):\n",
    "    def __init__(self, db_ip, db_port, db_name, table_name):\n",
    "        self.db_ip = db_ip\n",
    "        self.db_port = db_port\n",
    "        self.db_name = db_name\n",
    "        self.table_name = table_name\n",
    "        self.conn = MongoClient(host=self.db_ip, port=self.db_port)\n",
    "        self.db = self.conn[self.db_name]\n",
    "        self.table = self.db[self.table_name]\n",
    "    def get_one(self, query):\n",
    "        return self.table.find_one(query, projection={\"_id\": False})\n",
    "    def get_all(self, query):\n",
    "        return self.table.find(query)\n",
    "    def add(self, kv_dict):\n",
    "        return self.table.insert_one(kv_dict)\n",
    "    def delete(self, query):\n",
    "        return self.table.delete_many(query)\n",
    "    def check_exist(self, query):\n",
    "        ret = self.table.find_one(query)\n",
    "        return ret != None\n",
    "    # 如果没有会新建\n",
    "    def update(self, query, kv_dict):\n",
    "            self.table.update_one(query,{\n",
    "              '$set': kv_dict\n",
    "            }, upsert=True)\n",
    "\n",
    "link = \"https://bbs.hupu.com/bxj\"\n",
    "soup = get_page(link)\n",
    "post_all= soup.find('ul', class_=\"for-list\")\n",
    "post_list = post_all.find_all('li')\n",
    "data_list = get_data(post_list)\n",
    "for each in data_list:\n",
    "    print (each)\n",
    "    \n",
    "hupu_post = MongoAPI(\"localhost\",  27017,  \"hupu\", \"post\")\n",
    "for each in data_list:\n",
    "    hupu_post.update({\"post_link\": each[1]},{\"title\": each[0], \n",
    "                    \"post_link\": each[1],\n",
    "                   \"author\": each[2],\n",
    "                   \"author_page\": each[3],\n",
    "                   \"start_date\": str(each[4]),\n",
    "                   \"reply\": each[5],\n",
    "                   \"view\": each[6],\n",
    "                   \"last_reply\": each[7],\n",
    "                   \"last_reply_time\": str(each[8])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
